{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Intermediate-Advanced Shell for Bioinformatics \u00b6 Lesson Overview 1. UNIX,Linux & UNIX Shell Introduction to UNIX operating system, Linux and UNIX Shell 2. UNIX Shell Basics & Recap Navigating Files & Directories and a review of commands used in routine tasks 3. Download and verify data Downloading data with wget / curl and check the transferred data\u2019s integrity with check\u2010sums 4. Streams, Redirection and Pipe Combining pipes and redirection, Using \"Exit\" statuses 5. Inspecting and Manipulating Text Data with UNIX Tools - Part 1 Inspect file/s with utilities such as head , less . Extracting and formatting tabular data. Magical grep . 6. Inspecting and Manipulating Text Data with UNIX Tools - Part 2 Substitute matching patterns with sed . Text processing with awk and bioawk 7. Automating File-Processing with find and xargs Search files by pattern with find and use xargs to execute a command for those objects matching the pattern 8. Puzzles \ud83e\udde9 Can you use shell scripts to solve these \"real\" life challenged in molecular biology ? 9. Supplementary_1 Escaping, Special Characters Attribution Notice \u00b6 This workshop material is heavily inspired by : Buffalo, V (2015). Bioinformatics Data Skills .O'Reilly Media, Inc The Carpentries. The Unix Shell . https://swcarpentry.github.io/shell-novice/ The Carpentries. Introduction to Command Line for Genomics . https://datacarpentry.org/shell-genomics/ Rosalind Project. https://rosalind.info/about/ License \u00b6 Genomics Aotearoa / New Zealand eScience Infrastructure \"Intermediate-Advanced Shell for Bioinformatics\" is licensed under the GNU General Public License v3.0, 29 June 2007 . ( Follow this link for more information ) Setup \u00b6 If possible, we do recommend using the Remote option over Local ( Especially for Windows hosts). This will eliminate the need to install any additional applications Remote option will require an existing NeSI Account Remote \u00b6 Log into NeSI Mahuika Jupyter Service Follow https://jupyter.nesi.org.nz/hub/login Enter NeSI username, HPC password and 6 digit second factor token Choose server options as below >>make sure to choose the correct project code nesi02659 , number of CPUs CPUs=4 , memory 8 GB prior to pressing button. Local \u00b6 Local host setup - Windows, MacOS & Linux Windows Hosts MacOS Linux Install either Git for Windows from https://git-scm.com/download/win OR MobaXterm Home ( Portable or Installer edition) from https://mobaxterm.mobatek.net/download-home-edition.html Portable edition does not require administrative privileges Native terminal client is sufficient. It might not comes with wget download data via command line (can be installed with $ brew install wget ) However, it is not required as we provide a direct link to download data in .zip format Native terminal client is sufficient. bioawk install on all hosts One of the tools used in this workshop is bioawk which is not a native Linu/UNIX utility. Installing it on MacOS and Linux can be done with $ brew install bioawk & $ sudo apt install bioawk , respectively. Windows hosts might have to do it via conda according to these instructions . However, this will require a prior install of Anaconda Or Miniconda","title":"Home"},{"location":"#intermediate-advanced-shell-for-bioinformatics","text":"Lesson Overview 1. UNIX,Linux & UNIX Shell Introduction to UNIX operating system, Linux and UNIX Shell 2. UNIX Shell Basics & Recap Navigating Files & Directories and a review of commands used in routine tasks 3. Download and verify data Downloading data with wget / curl and check the transferred data\u2019s integrity with check\u2010sums 4. Streams, Redirection and Pipe Combining pipes and redirection, Using \"Exit\" statuses 5. Inspecting and Manipulating Text Data with UNIX Tools - Part 1 Inspect file/s with utilities such as head , less . Extracting and formatting tabular data. Magical grep . 6. Inspecting and Manipulating Text Data with UNIX Tools - Part 2 Substitute matching patterns with sed . Text processing with awk and bioawk 7. Automating File-Processing with find and xargs Search files by pattern with find and use xargs to execute a command for those objects matching the pattern 8. Puzzles \ud83e\udde9 Can you use shell scripts to solve these \"real\" life challenged in molecular biology ? 9. Supplementary_1 Escaping, Special Characters","title":"Intermediate-Advanced Shell for Bioinformatics"},{"location":"#attribution-notice","text":"This workshop material is heavily inspired by : Buffalo, V (2015). Bioinformatics Data Skills .O'Reilly Media, Inc The Carpentries. The Unix Shell . https://swcarpentry.github.io/shell-novice/ The Carpentries. Introduction to Command Line for Genomics . https://datacarpentry.org/shell-genomics/ Rosalind Project. https://rosalind.info/about/","title":"Attribution Notice"},{"location":"#license","text":"Genomics Aotearoa / New Zealand eScience Infrastructure \"Intermediate-Advanced Shell for Bioinformatics\" is licensed under the GNU General Public License v3.0, 29 June 2007 . ( Follow this link for more information )","title":"License"},{"location":"#setup","text":"If possible, we do recommend using the Remote option over Local ( Especially for Windows hosts). This will eliminate the need to install any additional applications Remote option will require an existing NeSI Account","title":"Setup"},{"location":"#remote","text":"Log into NeSI Mahuika Jupyter Service Follow https://jupyter.nesi.org.nz/hub/login Enter NeSI username, HPC password and 6 digit second factor token Choose server options as below >>make sure to choose the correct project code nesi02659 , number of CPUs CPUs=4 , memory 8 GB prior to pressing button.","title":"Remote"},{"location":"#local","text":"Local host setup - Windows, MacOS & Linux Windows Hosts MacOS Linux Install either Git for Windows from https://git-scm.com/download/win OR MobaXterm Home ( Portable or Installer edition) from https://mobaxterm.mobatek.net/download-home-edition.html Portable edition does not require administrative privileges Native terminal client is sufficient. It might not comes with wget download data via command line (can be installed with $ brew install wget ) However, it is not required as we provide a direct link to download data in .zip format Native terminal client is sufficient. bioawk install on all hosts One of the tools used in this workshop is bioawk which is not a native Linu/UNIX utility. Installing it on MacOS and Linux can be done with $ brew install bioawk & $ sudo apt install bioawk , respectively. Windows hosts might have to do it via conda according to these instructions . However, this will require a prior install of Anaconda Or Miniconda","title":"Local"},{"location":"0_introduction/","text":"1. UNIX, Linux & UNIX Shell \u00b6 Lesson Objectives Quick overview on UNIX operating system and it's importance Differences/similarities between UNIX vs. Linux What is a shell and the importance of UNIX shell/s for Bioinformatics (or for Scientific computing in general) Types of Shell and intro to Bash Shell (we will be using the latter throughout the workshop) The UNIX operating system \u00b6 Unix is a multi-user operating system which allows more than one person to use the computer resources at a time. It was originally designed as a time-sharing system to serve several users simultaneous. Unix allows direct communication with the computer via a terminal, hence being very interactive and giving the user direct control over the computer resources. Unix also gives users the ability to share data and programs among one another. Unix is a generic operating system which takes full advantage of all available hardware such as 32-bit processor chips, expanded memory, and large, fast hard drives. Since Unix is written in a machine-independent language (C/C++) it is portable to many different types of machines including PC's. Therefore, Unix can be adapted to meet special requirements. The UNIX operating system is made up of three parts; the kernel, the shell and the programs. On Unix philosophy \u201cAlthough that philosophy can\u2019t be written down in a single sentence, as its heart is the idea that the power of a system comes more from the relationships among programs than from the programs themselves. Many UNIX programs do quite trivial things in isolation, but, combined with other programs, become general and useful tools.\u201d \u2013 Brian Kernighan & Rob Pike The UNIX operating system is made up of three parts; the kernel , the shell and the programs Kernel \u2212 The kernel is the heart of the operating system. It interacts with the hardware and most of the tasks like memory management, task scheduling and file management. Shell \u2212 The shell is the utility that processes your requests (acts as an interface between the user and the kernel). When you type in a command at your terminal, the shell interprets (operating as in interpreter ) the command and calls the program that you want. The shell uses standard syntax for all commands. The shell recognizes a limited set of commands, and you must give commands to the shell in a way that it understands: Each shell command consists of a command name, followed by command options (if any are desired) and command arguments (if any are desired). The command name, options, and arguments, are separated by blank space. An interpreter operates in a simple loop: It accepts a command, interprets the command, executes the command, and then waits for another command. The shell displays a \"prompt,\" to notify you that it is ready to accept your command. UNIX vs. Linux \u00b6 Linux is not Unix, but it is a \"Unix-like\" operating system. Linux system is derived from Unix and it is a continuation of the basis of Unix design. Linux distributions are the most famous and healthiest example of the direct Unix derivatives. BSD (Berkley Software Distribution) is also an example of a Unix derivative. Unix-like & a bit more on Linux A Unix-like OS (also called as UN*X or *nix) is the one that works in a way similar to Unix systems, however, it is not necessary that they conform to Single UNIX Specification (SUS) or similar POSIX (Portable Operating System Interface) standard. SUS is a standard which is required to be met for any OS (Operating System) to qualify for using \u2018UNIX\u2019 trademark. This trademark is granted by \u2018The Open Group\u2019 Some examples of currently registered UNIX systems include macOS, Solaris, and AIX. If we consider the POSIX system, then Linux can be regarded as Unix-like OS. Linux is just the kernel and not the complete OS. This Linux kernel is generally packaged in Linux distributions which thereby makes it a complete OS. Linux distribution (also called a distro ) is an operating system that is created from a collection of software built upon the Linux Kernel and is a package management system. A standard Linux distribution consists of a Linux kernel, GNU system, GNU utilities, libraries, compiler, additional software, documentation, a window system, window manager and a desktop environment. Most of the software included in a Linux distribution is free and open source. They may include some proprietary software like binary blobs which are essential for a few device drivers. UNIX Shell for Bioinformatics \u00b6 A shell is a computer program that presents a command line interface which allows you to control your computer using commands entered with a keyboard instead of controlling graphical user interfaces (GUIs) with a mouse/keyboard/touchscreen combination. There are many reasons to learn about the shell: Many bioinformatics tools can only be used through a command line interface. Many more have features and parameter options which are not available in the GUI. BLAST is an example. Many of the advanced functions are only accessible to users who know how to use a shell. The shell makes your work less boring. In bioinformatics you often need to repeat tasks with a large number of files. With the shell, you can automate those repetitive tasks and leave you free to do more exciting things. The shell makes your work less error-prone. When humans do the same thing a hundred different times (or even ten times), they\u2019re likely to make a mistake. Your computer can do the same thing a thousand times with no mistakes. The shell makes your work more reproducible. When you carry out your work in the command-line (rather than a GUI), your computer keeps a record of every step that you\u2019ve carried out which you can use to re-do your work when you need to. It also gives you a way to communicate unambiguously what you\u2019ve done, so that others can inspect or apply your process to new data. Many bioinformatic tasks require large amounts of computing power and can\u2019t realistically be run on your own machine. These tasks are best performed using remote computers or cloud computing, which can only be accessed through a shell. Different Types of Shells \u00b6 Being able to interact with the kernel makes shells a powerful tool. Without the ability to interact with the kernel, a user cannot access the utilities offered by their machine\u2019s operating system. Let\u2019s take a look at some of the major shells that are available for the Linux environment Types of Shells Bourne Shell (sh) C Shell (csh) Korn Shell (ksh) Z Shell (zsh) Fish Shell (fish) Developed at AT&T Bell Labs by Steve Bourne , the Bourne shell is regarded as the first UNIX shell ever. It is denoted as sh. It gained popularity due to its compact nature and high speeds of operation. The C shell was created at the University of California by Bill Joy. It is denoted as csh. It was developed to include useful programming features like in-built support for arithmetic operations and a syntax similar to the C programming language. Further, it incorporated command history which was missing in different types of shells in Linux like the Bourne shell. Another prominent feature of a C shell is \u201caliases\u201d. The complete path-name for the C shell is /bin/csh . By default, it uses the prompt hostname# for the root user and hostname% for the non-root users. The Korn shell was developed at AT&T Bell Labs by David Korn, to improve the Bourne shell. It is denoted as ksh. The Korn shell is essentially a superset of the Bourne shell. Besides supporting everything that would be supported by the Bourne shell, it provides users with new functionalities. It allows in-built support for arithmetic operations while offering interactive features which are similar to the C shell. The Korn shell runs scripts made for the Bourne shell, while offering string, array and function manipulation similar to the C programming language. It also supports scripts which were written for the C shell. Further, it is faster than most different types of shells. zsh is a shell designed for interactive use, although it is also a powerful scripting language. Many of the useful features of bash, ksh, and tcsh were incorporated into zsh; many original features were added. Fish is a fully-equipped command line shell (like bash or zsh) that is smart and user-friendly. Fish supports powerful features like syntax highlighting, autosuggestions, and tab completions that just work, with nothing to learn or configure. If you want to make your command line more productive, more useful, and more fun, without learning a bunch of arcane syntax and configuration options, then fish might be just what you\u2019re looking for! Type of Shell for this workshop: GNU Bourne-Again shell (bash) \u00b6 The GNU Bourne-Again shell was designed to be compatible with the Bourne shell. It incorporates useful features from different types of shells in Linux such as Korn shell and C shell. The shell's name bash is an acronym for \"Bourne Again Shell\", a pun on the name of the Bourne shell that it replaces and the notion of being \"born again\" First released in 1989, it has been used as the default login shell for most Linux distributions. Bash was also the default shell in all versions of Apple macOS prior to the 2019 release of macOS Catalina, which changed the default shell to zsh, although Bash remains available as an alternative shell The Bash command syntax is a superset of the Bourne shell command syntax. Bash supports brace expansion, command line completion (Programmable Completion), basic debugging and signal handling (using trap ) among other features. Bash can execute the vast majority of Bourne shell scripts without modification, with the exception of Bourne shell scripts stumbling into fringe syntax behavior interpreted differently in Bash or attempting to run a system command matching a newer Bash builtin, etc. Back to homepage","title":"1. UNIX, Linux  & UNIX Shell"},{"location":"0_introduction/#1-unix-linux-unix-shell","text":"Lesson Objectives Quick overview on UNIX operating system and it's importance Differences/similarities between UNIX vs. Linux What is a shell and the importance of UNIX shell/s for Bioinformatics (or for Scientific computing in general) Types of Shell and intro to Bash Shell (we will be using the latter throughout the workshop)","title":"1. UNIX, Linux  &amp; UNIX Shell"},{"location":"0_introduction/#the-unix-operating-system","text":"Unix is a multi-user operating system which allows more than one person to use the computer resources at a time. It was originally designed as a time-sharing system to serve several users simultaneous. Unix allows direct communication with the computer via a terminal, hence being very interactive and giving the user direct control over the computer resources. Unix also gives users the ability to share data and programs among one another. Unix is a generic operating system which takes full advantage of all available hardware such as 32-bit processor chips, expanded memory, and large, fast hard drives. Since Unix is written in a machine-independent language (C/C++) it is portable to many different types of machines including PC's. Therefore, Unix can be adapted to meet special requirements. The UNIX operating system is made up of three parts; the kernel, the shell and the programs. On Unix philosophy \u201cAlthough that philosophy can\u2019t be written down in a single sentence, as its heart is the idea that the power of a system comes more from the relationships among programs than from the programs themselves. Many UNIX programs do quite trivial things in isolation, but, combined with other programs, become general and useful tools.\u201d \u2013 Brian Kernighan & Rob Pike The UNIX operating system is made up of three parts; the kernel , the shell and the programs Kernel \u2212 The kernel is the heart of the operating system. It interacts with the hardware and most of the tasks like memory management, task scheduling and file management. Shell \u2212 The shell is the utility that processes your requests (acts as an interface between the user and the kernel). When you type in a command at your terminal, the shell interprets (operating as in interpreter ) the command and calls the program that you want. The shell uses standard syntax for all commands. The shell recognizes a limited set of commands, and you must give commands to the shell in a way that it understands: Each shell command consists of a command name, followed by command options (if any are desired) and command arguments (if any are desired). The command name, options, and arguments, are separated by blank space. An interpreter operates in a simple loop: It accepts a command, interprets the command, executes the command, and then waits for another command. The shell displays a \"prompt,\" to notify you that it is ready to accept your command.","title":"The UNIX operating system"},{"location":"0_introduction/#unix-vs-linux","text":"Linux is not Unix, but it is a \"Unix-like\" operating system. Linux system is derived from Unix and it is a continuation of the basis of Unix design. Linux distributions are the most famous and healthiest example of the direct Unix derivatives. BSD (Berkley Software Distribution) is also an example of a Unix derivative. Unix-like & a bit more on Linux A Unix-like OS (also called as UN*X or *nix) is the one that works in a way similar to Unix systems, however, it is not necessary that they conform to Single UNIX Specification (SUS) or similar POSIX (Portable Operating System Interface) standard. SUS is a standard which is required to be met for any OS (Operating System) to qualify for using \u2018UNIX\u2019 trademark. This trademark is granted by \u2018The Open Group\u2019 Some examples of currently registered UNIX systems include macOS, Solaris, and AIX. If we consider the POSIX system, then Linux can be regarded as Unix-like OS. Linux is just the kernel and not the complete OS. This Linux kernel is generally packaged in Linux distributions which thereby makes it a complete OS. Linux distribution (also called a distro ) is an operating system that is created from a collection of software built upon the Linux Kernel and is a package management system. A standard Linux distribution consists of a Linux kernel, GNU system, GNU utilities, libraries, compiler, additional software, documentation, a window system, window manager and a desktop environment. Most of the software included in a Linux distribution is free and open source. They may include some proprietary software like binary blobs which are essential for a few device drivers.","title":"UNIX vs. Linux"},{"location":"0_introduction/#unix-shell-for-bioinformatics","text":"A shell is a computer program that presents a command line interface which allows you to control your computer using commands entered with a keyboard instead of controlling graphical user interfaces (GUIs) with a mouse/keyboard/touchscreen combination. There are many reasons to learn about the shell: Many bioinformatics tools can only be used through a command line interface. Many more have features and parameter options which are not available in the GUI. BLAST is an example. Many of the advanced functions are only accessible to users who know how to use a shell. The shell makes your work less boring. In bioinformatics you often need to repeat tasks with a large number of files. With the shell, you can automate those repetitive tasks and leave you free to do more exciting things. The shell makes your work less error-prone. When humans do the same thing a hundred different times (or even ten times), they\u2019re likely to make a mistake. Your computer can do the same thing a thousand times with no mistakes. The shell makes your work more reproducible. When you carry out your work in the command-line (rather than a GUI), your computer keeps a record of every step that you\u2019ve carried out which you can use to re-do your work when you need to. It also gives you a way to communicate unambiguously what you\u2019ve done, so that others can inspect or apply your process to new data. Many bioinformatic tasks require large amounts of computing power and can\u2019t realistically be run on your own machine. These tasks are best performed using remote computers or cloud computing, which can only be accessed through a shell.","title":"UNIX Shell for Bioinformatics"},{"location":"0_introduction/#different-types-of-shells","text":"Being able to interact with the kernel makes shells a powerful tool. Without the ability to interact with the kernel, a user cannot access the utilities offered by their machine\u2019s operating system. Let\u2019s take a look at some of the major shells that are available for the Linux environment Types of Shells Bourne Shell (sh) C Shell (csh) Korn Shell (ksh) Z Shell (zsh) Fish Shell (fish) Developed at AT&T Bell Labs by Steve Bourne , the Bourne shell is regarded as the first UNIX shell ever. It is denoted as sh. It gained popularity due to its compact nature and high speeds of operation. The C shell was created at the University of California by Bill Joy. It is denoted as csh. It was developed to include useful programming features like in-built support for arithmetic operations and a syntax similar to the C programming language. Further, it incorporated command history which was missing in different types of shells in Linux like the Bourne shell. Another prominent feature of a C shell is \u201caliases\u201d. The complete path-name for the C shell is /bin/csh . By default, it uses the prompt hostname# for the root user and hostname% for the non-root users. The Korn shell was developed at AT&T Bell Labs by David Korn, to improve the Bourne shell. It is denoted as ksh. The Korn shell is essentially a superset of the Bourne shell. Besides supporting everything that would be supported by the Bourne shell, it provides users with new functionalities. It allows in-built support for arithmetic operations while offering interactive features which are similar to the C shell. The Korn shell runs scripts made for the Bourne shell, while offering string, array and function manipulation similar to the C programming language. It also supports scripts which were written for the C shell. Further, it is faster than most different types of shells. zsh is a shell designed for interactive use, although it is also a powerful scripting language. Many of the useful features of bash, ksh, and tcsh were incorporated into zsh; many original features were added. Fish is a fully-equipped command line shell (like bash or zsh) that is smart and user-friendly. Fish supports powerful features like syntax highlighting, autosuggestions, and tab completions that just work, with nothing to learn or configure. If you want to make your command line more productive, more useful, and more fun, without learning a bunch of arcane syntax and configuration options, then fish might be just what you\u2019re looking for!","title":"Different Types of Shells"},{"location":"0_introduction/#type-of-shell-for-this-workshop-gnu-bourne-again-shell-bash","text":"The GNU Bourne-Again shell was designed to be compatible with the Bourne shell. It incorporates useful features from different types of shells in Linux such as Korn shell and C shell. The shell's name bash is an acronym for \"Bourne Again Shell\", a pun on the name of the Bourne shell that it replaces and the notion of being \"born again\" First released in 1989, it has been used as the default login shell for most Linux distributions. Bash was also the default shell in all versions of Apple macOS prior to the 2019 release of macOS Catalina, which changed the default shell to zsh, although Bash remains available as an alternative shell The Bash command syntax is a superset of the Bourne shell command syntax. Bash supports brace expansion, command line completion (Programmable Completion), basic debugging and signal handling (using trap ) among other features. Bash can execute the vast majority of Bourne shell scripts without modification, with the exception of Bourne shell scripts stumbling into fringe syntax behavior interpreted differently in Bash or attempting to run a system command matching a newer Bash builtin, etc. Back to homepage","title":"Type of Shell for this workshop: GNU Bourne-Again shell (bash)"},{"location":"1_unixshellbasics/","text":"3. Unix Shell Basics & Recap \u00b6 Lesson Objectives \ud83d\ude36\u200d\ud83c\udf2b\ufe0f Navigate your file system using the command line. Quick recap on commands used in routine tasks such copy, move, remove. It is expected that you are already familiar with using the basics of the Unix Shell. As a quick refresher, some frequently used commands are listed below. For more information about a command, use the Unix man (manual) command. For example, to get more information about the mkdir command, type: man mkdir Key commands for navigating around the filesystem are: ls - list the contents of the current directory ls -l - list the contents of the current directory in more detail pwd - show the location of the current directory cd DIR - change directory to directory DIR (DIR must be in your current directory - you should see its name when you type ls OR you need to specify either a full or relative path to DIR) cd - - change back to the last directory you were in cd (also cd ~/ ) - change to your home directory cd .. - change to the directory one level above Other useful commands: mv - move files or directories cp - copy files or directories rm - delete files or directories mkdir - create a new directory cat - concatenate and print text files to screen more - show contents of text files on screen less - cooler version of more . Allows searching (use / ) tree - tree view of directory structure head - view lines from the start of a file tail - view lines from the end of a file grep - find patterns within files Back to homepage","title":"3. Unix Shell Basics & Recap"},{"location":"1_unixshellbasics/#3-unix-shell-basics-recap","text":"Lesson Objectives \ud83d\ude36\u200d\ud83c\udf2b\ufe0f Navigate your file system using the command line. Quick recap on commands used in routine tasks such copy, move, remove. It is expected that you are already familiar with using the basics of the Unix Shell. As a quick refresher, some frequently used commands are listed below. For more information about a command, use the Unix man (manual) command. For example, to get more information about the mkdir command, type: man mkdir Key commands for navigating around the filesystem are: ls - list the contents of the current directory ls -l - list the contents of the current directory in more detail pwd - show the location of the current directory cd DIR - change directory to directory DIR (DIR must be in your current directory - you should see its name when you type ls OR you need to specify either a full or relative path to DIR) cd - - change back to the last directory you were in cd (also cd ~/ ) - change to your home directory cd .. - change to the directory one level above Other useful commands: mv - move files or directories cp - copy files or directories rm - delete files or directories mkdir - create a new directory cat - concatenate and print text files to screen more - show contents of text files on screen less - cooler version of more . Allows searching (use / ) tree - tree view of directory structure head - view lines from the start of a file tail - view lines from the end of a file grep - find patterns within files Back to homepage","title":"3. Unix Shell Basics &amp; Recap"},{"location":"2_download_data/","text":"2. Download & Verify Data \u00b6 Lesson Objectives To be able to download data with command line utilities Inspect data (checksum verification) Download with wget from a terminal client wget -c https://github.com/GenomicsAotearoa/shell-for-bioinformatics/releases/download/v1.0/shell4b_data.tar.gz OR wget -c https://github.com/GenomicsAotearoa/shell-for-bioinformatics/releases/download/v1.0/shell4b_data.zip Download via Web Browser (use locally) Data can be downloaded directly from this link which will download shell4b_data.tar.gz to Downloads directory If the above link fails, try this alternative .zip Decompress downloaded tar.gz OR .zip \u00b6 A TAR.GZ file is a combination of two different packaging algorithms. The first is tar , short for tape archive. It\u2019s an old utility invented mainly for accurate data transfer to devices without their own file systems. A tar file (or tarball) contains files in a sequential format, along with metadata about the directory structure and other technical parameters. It is useful to note that tar doesn\u2019t compress the files in question, only packages them. Indeed, sometimes the resulting tarball can be of greater size due to padding. That\u2019s where Gzip comes in. Gzip (denoted by a .gz file extension) is a compressed file format used to archive data to take up smaller space. Gzip uses the same compression algorithm as the more commonly known zip but can only be used on a single file. In short, Gzip compresses all the individual files and tar packages them in a single archive. Decompressing the tar.gz file can be done with the built-in tar utility tar -xvzf shell4b_data.tar.gz x : Extract an archive. z : Compress the archive with gzip. v : Display progress in the terminal while creating the archive, also known as \u201cverbose\u201d mode. The v is always optional in these commands, but it\u2019s helpful. f : Allows you to specify the filename of the archive. ZIP files can store multiple files using different compression techniques while at the same time supports storing a file without any compression. Each file is stored/compressed individually which helps to extract them, or add new ones, without applying compression or decompression to the entire archive. Each file in a ZIP archive is represented as an individual entry consisting of a Local File Header followed by the compressed file data. The Directory at the end of the archive holds the references to all these file entries. ZIP file readers should avoid reading the local file headers and all types of file listing should be read from the Directory. This Directory is the only source for valid file entries in the archive as files can be appended towards the end of the archive as well. That is why if a reader reads local headers of a ZIP archive from the beginning, it may read invalid (deleted) entries as well those are not part of the Directory being deleted from archive. The order of the file entries in the central directory need not coincide with the order of file entries in the archive. Decompressing .zip files can be done with unzip command unzip -v shell4b_data.zip v : Display progress in the terminal while creating the archive, also known as \u201cverbose\u201d mode. The v is always optional in these commands, but it\u2019s helpful. Data Integrity \u00b6 Data we download is the starting point of all future analyses and conclusions. Therefore it\u2019s important to explicitly check the transferred data\u2019s integrity with check\u2010sums. Checksums are very compressed summaries of data, computed in a way that even if just one bit of the data is changed the checksum will be different. As such, data integrity checks are also helpful in keeping track of data versions. Checksums facilitate reproducibility, as we can link a particular analysis and set of results to an exact version of data summarized by the data\u2019s checksum value. SHA and MD5 Checksums Two most common checksum algorithms are MD5 and SHA (Specifically referring to SHA256 ) MD5 : cryptographic hash function algorithm that takes the message as input of any length and changes it into a fixed-length message of 16 bytes. MD5 algorithm stands for the message-digest algorithm. SHA : SHA-256 is a more secure and newer cryptographic hash function that was launched in 2000 as a new version of SHA functions and was adopted as Federal Information Processing Standards (FIPS) in 2002. It is allowed to use a hash generator tool to produce a SHA256 hash for any string or input value. Also, it generates 256 hash values, and the internal state size is 256 bit and the original message size is up to 264-1 bits. To create checksums, we can pass arbitrary strings to the program md5sum (or sha256sum ) through standard in echo \"shell for Bioinformatics\" | md5sum echo \"shell for BioInformatics\" | md5sum 198638c380be53bf3f6ff70d5626ae44 - afa4dbcc56b540e24558085fdc10342f - Checksums are reported in hexadecimal format, where each digit can be one of 16 characters: digits 0 through 9, and the letters a, b, c, d, e, and f. The trailing dash indicates this is the MD5 checksum of input from standard in. Checksums with file input can be done with md5usm filename .i.e. md5sum tb1.fasta f44dca62012017196b545a2dd2d2906d tb1.fasta Because it can get rather tedious to check each checksum individually, both md5sum and sha256sum has a convenient solution: Let's say that we want to verify the checksums for all of the .fasta files in the current directory. This can be done by creating and validating against a file containing the checksums of all the .fasta files. fasta_checksums.md5 contains the checksums for three .fasta files which were verified prior to uploading to github repo. This was generated by using the command md5sum *.fasta > fasta_checksums.md5 Run md5sum -c fasta_checksums.md5 and see whether all the .fasta files pass the verification -c option represent check True story - Applications will not trigger clear error messages for corrupted data The following is an error message recorded on the log for a failed bedtools genomecov process ran on NeSI Mahuika cluster terminate called after throwing an instance of 'std::bad_alloc' what () : std::bad_alloc Doing a Google search for std::bad_alloc will take us to this official reference documentation Expand the search a bit more with BedTools std::bad_alloc which will return this reported issue on Biostars as the first result. std::bad_alloc is the C++ error code for when a new operator tries to allocate something, but fails. As C++ cannot dynamically allocate memory, the lack of memory is the most common cause. So, it's quite difficult avoid the temptation to throw more memory at this process and re-run it. In fact, it was re-run with 0.5TB of memory as a sanity check which triggered the same error. As it turned out, .bed file was corrupted . \ud83d\udd75\ufe0f\u200d\u2640\ufe0f","title":"2. Download & Verify Data"},{"location":"2_download_data/#2-download-verify-data","text":"Lesson Objectives To be able to download data with command line utilities Inspect data (checksum verification) Download with wget from a terminal client wget -c https://github.com/GenomicsAotearoa/shell-for-bioinformatics/releases/download/v1.0/shell4b_data.tar.gz OR wget -c https://github.com/GenomicsAotearoa/shell-for-bioinformatics/releases/download/v1.0/shell4b_data.zip Download via Web Browser (use locally) Data can be downloaded directly from this link which will download shell4b_data.tar.gz to Downloads directory If the above link fails, try this alternative .zip","title":"2. Download &amp; Verify Data"},{"location":"2_download_data/#decompress-downloaded-targz-or-zip","text":"A TAR.GZ file is a combination of two different packaging algorithms. The first is tar , short for tape archive. It\u2019s an old utility invented mainly for accurate data transfer to devices without their own file systems. A tar file (or tarball) contains files in a sequential format, along with metadata about the directory structure and other technical parameters. It is useful to note that tar doesn\u2019t compress the files in question, only packages them. Indeed, sometimes the resulting tarball can be of greater size due to padding. That\u2019s where Gzip comes in. Gzip (denoted by a .gz file extension) is a compressed file format used to archive data to take up smaller space. Gzip uses the same compression algorithm as the more commonly known zip but can only be used on a single file. In short, Gzip compresses all the individual files and tar packages them in a single archive. Decompressing the tar.gz file can be done with the built-in tar utility tar -xvzf shell4b_data.tar.gz x : Extract an archive. z : Compress the archive with gzip. v : Display progress in the terminal while creating the archive, also known as \u201cverbose\u201d mode. The v is always optional in these commands, but it\u2019s helpful. f : Allows you to specify the filename of the archive. ZIP files can store multiple files using different compression techniques while at the same time supports storing a file without any compression. Each file is stored/compressed individually which helps to extract them, or add new ones, without applying compression or decompression to the entire archive. Each file in a ZIP archive is represented as an individual entry consisting of a Local File Header followed by the compressed file data. The Directory at the end of the archive holds the references to all these file entries. ZIP file readers should avoid reading the local file headers and all types of file listing should be read from the Directory. This Directory is the only source for valid file entries in the archive as files can be appended towards the end of the archive as well. That is why if a reader reads local headers of a ZIP archive from the beginning, it may read invalid (deleted) entries as well those are not part of the Directory being deleted from archive. The order of the file entries in the central directory need not coincide with the order of file entries in the archive. Decompressing .zip files can be done with unzip command unzip -v shell4b_data.zip v : Display progress in the terminal while creating the archive, also known as \u201cverbose\u201d mode. The v is always optional in these commands, but it\u2019s helpful.","title":"Decompress downloaded tar.gz OR .zip"},{"location":"2_download_data/#data-integrity","text":"Data we download is the starting point of all future analyses and conclusions. Therefore it\u2019s important to explicitly check the transferred data\u2019s integrity with check\u2010sums. Checksums are very compressed summaries of data, computed in a way that even if just one bit of the data is changed the checksum will be different. As such, data integrity checks are also helpful in keeping track of data versions. Checksums facilitate reproducibility, as we can link a particular analysis and set of results to an exact version of data summarized by the data\u2019s checksum value. SHA and MD5 Checksums Two most common checksum algorithms are MD5 and SHA (Specifically referring to SHA256 ) MD5 : cryptographic hash function algorithm that takes the message as input of any length and changes it into a fixed-length message of 16 bytes. MD5 algorithm stands for the message-digest algorithm. SHA : SHA-256 is a more secure and newer cryptographic hash function that was launched in 2000 as a new version of SHA functions and was adopted as Federal Information Processing Standards (FIPS) in 2002. It is allowed to use a hash generator tool to produce a SHA256 hash for any string or input value. Also, it generates 256 hash values, and the internal state size is 256 bit and the original message size is up to 264-1 bits. To create checksums, we can pass arbitrary strings to the program md5sum (or sha256sum ) through standard in echo \"shell for Bioinformatics\" | md5sum echo \"shell for BioInformatics\" | md5sum 198638c380be53bf3f6ff70d5626ae44 - afa4dbcc56b540e24558085fdc10342f - Checksums are reported in hexadecimal format, where each digit can be one of 16 characters: digits 0 through 9, and the letters a, b, c, d, e, and f. The trailing dash indicates this is the MD5 checksum of input from standard in. Checksums with file input can be done with md5usm filename .i.e. md5sum tb1.fasta f44dca62012017196b545a2dd2d2906d tb1.fasta Because it can get rather tedious to check each checksum individually, both md5sum and sha256sum has a convenient solution: Let's say that we want to verify the checksums for all of the .fasta files in the current directory. This can be done by creating and validating against a file containing the checksums of all the .fasta files. fasta_checksums.md5 contains the checksums for three .fasta files which were verified prior to uploading to github repo. This was generated by using the command md5sum *.fasta > fasta_checksums.md5 Run md5sum -c fasta_checksums.md5 and see whether all the .fasta files pass the verification -c option represent check True story - Applications will not trigger clear error messages for corrupted data The following is an error message recorded on the log for a failed bedtools genomecov process ran on NeSI Mahuika cluster terminate called after throwing an instance of 'std::bad_alloc' what () : std::bad_alloc Doing a Google search for std::bad_alloc will take us to this official reference documentation Expand the search a bit more with BedTools std::bad_alloc which will return this reported issue on Biostars as the first result. std::bad_alloc is the C++ error code for when a new operator tries to allocate something, but fails. As C++ cannot dynamically allocate memory, the lack of memory is the most common cause. So, it's quite difficult avoid the temptation to throw more memory at this process and re-run it. In fact, it was re-run with 0.5TB of memory as a sanity check which triggered the same error. As it turned out, .bed file was corrupted . \ud83d\udd75\ufe0f\u200d\u2640\ufe0f","title":"Data Integrity"},{"location":"3_streams_red_pipe/","text":"4. Streams, Redirection and Pipe \u00b6 Lesson Objectives To be able to redirect streams of data in Unix. Solve problems by piping several Unix commands. Command substitution Bioinformatics data is often text-based and large. This is why Unix\u2019s philosophy of handling text streams is useful in bioinformatics: text streams allow us to do processing on a stream of data rather than holding it all in memory. Handling and redirecting the streams of data is essential skill in Unix. By default, both standard error and standard output of most unix programs go to your terminal screen. We can change this behavior (redirect the streams to a file) by using > or >> operators. The operator > redirects standard output to a file and overwrites any existing contents of the file, whereas >> appends to the file. If there isn\u2019t an existing file, both operators will create it before redirecting output to it. For example, to concatenate two FASTA files, we use cat command, but redirect the output to a file: Output redirection \u00b6 The shell4b_data directory contains the following fasta files: tb1.fasta tb1-protein.fasta tga1-protein.fasta We can use the cat command to view these files either one at a time: cat tb1-protein.fasta >teosinte-branched-1 protein LGVPSVKHMFPFCDSSSPMDLPLYQQLQLSPSSPKTDQSSSFYCYPCSPP FAAADASFPLSYQIGSAAAADATPPQAVINSPDLPVQALMDHAPAPATEL GACASGAEGSGASLDRAAAAARKDRHSKICTAGGMRDRRMRLSLDVARKF FALQDMLGFDKASKTVQWLLNTSKSAIQEIMADDASSECVEDGSSSLSVD GKHNPAEQLGGGGDQKPKGNCRGEGKKPAKASKAAATPKPPRKSANNAHQ VPDKETRAKARERARERTKEKHRMRWVKLASAIDVEAAAASVPSDRPSSN NLSHHSSLSMNMPCAAA cat tga1-protein.fasta >teosinte-glume-architecture-1 protein DSDCALSLLSAPANSSGIDVSRMVRPTEHVPMAQQPVVPGLQFGSASWFP RPQASTGGSFVPSCPAAVEGEQQLNAVLGPNDSEVSMNYGGMFHVGGGSG GGEGSSDGGT OR all at once with cat *.fasta We can also redirect the output to create a new file containing the sequence for both proteins: cat tb1-protein.fasta tga1-protein.fasta > zea-proteins.fasta Now we have a new file called zea-proteins.fasta . Let's check the contents: cat zea-proteins.fasta >teosinte-branched-1 protein LGVPSVKHMFPFCDSSSPMDLPLYQQLQLSPSSPKTDQSSSFYCYPCSPP FAAADASFPLSYQIGSAAAADATPPQAVINSPDLPVQALMDHAPAPATEL GACASGAEGSGASLDRAAAAARKDRHSKICTAGGMRDRRMRLSLDVARKF FALQDMLGFDKASKTVQWLLNTSKSAIQEIMADDASSECVEDGSSSLSVD GKHNPAEQLGGGGDQKPKGNCRGEGKKPAKASKAAATPKPPRKSANNAHQ VPDKETRAKARERARERTKEKHRMRWVKLASAIDVEAAAASVPSDRPSSN NLSHHSSLSMNMPCAAA >teosinte-glume-architecture-1 protein DSDCALSLLSAPANSSGIDVSRMVRPTEHVPMAQQPVVPGLQFGSASWFP RPQASTGGSFVPSCPAAVEGEQQLNAVLGPNDSEVSMNYGGMFHVGGGSG GGEGSSDGGT Capturing error messages cat tb1-protein.fasta mik.fasta >teosinte-branched-1 protein LGVPSVKHMFPFCDSSSPMDLPLYQQLQLSPSSPKTDQSSSFYCYPCSPP FAAADASFPLSYQIGSAAAADATPPQAVINSPDLPVQALMDHAPAPATEL GACASGAEGSGASLDRAAAAARKDRHSKICTAGGMRDRRMRLSLDVARKF FALQDMLGFDKASKTVQWLLNTSKSAIQEIMADDASSECVEDGSSSLSVD GKHNPAEQLGGGGDQKPKGNCRGEGKKPAKASKAAATPKPPRKSANNAHQ VPDKETRAKARERARERTKEKHRMRWVKLASAIDVEAAAASVPSDRPSSN NLSHHSSLSMNMPCAAA cat: mik.fasta: No such file or directory There are two different types of output there: \"standard output\" (the contents of the tb1-protein.fasta file) and standard error (the error message relating to the missing mik.fasta file). If we use the > operator to redirect the output, the standard output is captured, but the standard error is not - it is still printed to the screen. Let's check: cat tb1-protein.fasta mik.fasta > test.fasta cat: mik.fasta: No such file or directory The new file has been created and contains the standard output (contents of the file tb1-protein.fasta ): cat test.fasta >teosinte-branched-1 protein LGVPSVKHMFPFCDSSSPMDLPLYQQLQLSPSSPKTDQSSSFYCYPCSPP FAAADASFPLSYQIGSAAAADATPPQAVINSPDLPVQALMDHAPAPATEL GACASGAEGSGASLDRAAAAARKDRHSKICTAGGMRDRRMRLSLDVARKF FALQDMLGFDKASKTVQWLLNTSKSAIQEIMADDASSECVEDGSSSLSVD GKHNPAEQLGGGGDQKPKGNCRGEGKKPAKASKAAATPKPPRKSANNAHQ VPDKETRAKARERARERTKEKHRMRWVKLASAIDVEAAAASVPSDRPSSN NLSHHSSLSMNMPCAAA If we want to capture the standard error we use the (slightly unweildy) 2> operator: cat tb1-protein.fasta mik.fasta > test.fasta 2 > stderror.txt Descriptors File descriptor 2 represents standard error (other special file descriptors include 0 for standard input and 1 for standard output). Check the contents: cat stderror.txt cat: mik.fasta: No such file or directory Note that > will overwrite an existing file. We can use >> to add to a file instead of overwriting it: cat tga1-protein.fasta >> test.fasta cat test.fasta >teosinte-branched-1 protein LGVPSVKHMFPFCDSSSPMDLPLYQQLQLSPSSPKTDQSSSFYCYPCSPP FAAADASFPLSYQIGSAAAADATPPQAVINSPDLPVQALMDHAPAPATEL GACASGAEGSGASLDRAAAAARKDRHSKICTAGGMRDRRMRLSLDVARKF FALQDMLGFDKASKTVQWLLNTSKSAIQEIMADDASSECVEDGSSSLSVD GKHNPAEQLGGGGDQKPKGNCRGEGKKPAKASKAAATPKPPRKSANNAHQ VPDKETRAKARERARERTKEKHRMRWVKLASAIDVEAAAASVPSDRPSSN NLSHHSSLSMNMPCAAA >teosinte-glume-architecture-1 protein DSDCALSLLSAPANSSGIDVSRMVRPTEHVPMAQQPVVPGLQFGSASWFP RPQASTGGSFVPSCPAAVEGEQQLNAVLGPNDSEVSMNYGGMFHVGGGSG GGEGSSDGGT The Unix pipe \u00b6 The pipe operator ( | ) passes the output from one command to another command as input. The following is an example of using a pipe with the grep command. Steps: Remove the header information for the sequence (line starts with \">\") Highlight any characters in the sequence that are not A, T, C or G. We will use grep to carry out the first step, and then use the pipe operator to pass the output to a second grep command to carry out the second step. Here is the full command: grep -v \"^>\" tb1.fasta | grep --color -i \"[^ATCG]\" Let's see what each piece does grep -v \"^>\" tb1.fasta The -v tells grep to search for all lines in the file tb1.fasta that do not contain a \">\" at the start ( ^ is a special character that denotes \"at the start of the line - we'll learn more about this later). grep --color -i \"[^ATCG]\" There are a few things going on here: --color : tells grep to highlight any matches -i : tells grep to ignore the case (i.e., will match lower or upper case) [^ATCG] : when ^ is used inside square brackets it has a different function - inverts the pattern, so that grep finds any letters that are not A, T, C or G. Let's run the code: grep -v \"^>\" tb1.fasta | grep --color -i \"[^ATCG]\" CCCCAAAGACGGACCAATCCAGCAGCTTCTACTGCTA Y CCATGCTCCCCTCCCTTCGCCGCCGCCGACGC What if we had just run the code for step 2 on the tb1.fasta file? grep --color -i \"[^ATCG]\" tb1.fasta Combining pipes and redirection \u00b6 grep -v \"^>\" tb1.fasta | grep --color -i \"[^ATCG]\" > non-atcg.txt cat non-atcg.txt since we are redirecting to a text file, the --color by itself will not record the colour information. We can achieve this by invoking always flag for --color .i.e.. grep -v \"^>\" tb1.fasta | grep --color = always -i \"[^ATCG]\" > non-atcg.txt Using tee to capture intermediate outputs \u00b6 grep -v \"^>\" tb1.fasta | tee intermediate-file.txt | grep --color = always -i \"[^ATCG]\" > non-atcg.txt The file intermediate-file.txt will contain the output from grep -v \"^>\" tb1.fasta , but tee also passes that output through the pipe to the next grep command. Pipes and Chains and Long running processes : Exit Status (Programmatically Tell Whether Your Command Worked) \u00b6 How do you know when they complete? How do you know if they successfully finished without an error? Unix programs exit with an exit status, which indicates whether a program terminated without a problem or with an error. By Unix standards, an exit status of 0 indicates the process ran successfully, and any nonzero status indicates some sort of error has occurred (and hopefully the program prints an understandable error message, too). The exit status isn\u2019t printed to the terminal, but your shell will set its value to a shell variable named $? . We can use the echo command to look at this variable\u2019s value after running a command: program input.txt > results.txt ; echo $? Exit statuses are useful because they allow us to programmatically chain commands together in the shell. A subsequent command in a chain is run conditionally on the last command\u2019s exit status. The shell provides two operators that implement this: one operator that runs the subsequent command only if the first command completed successfully ( && ), and one operator that runs the next command only if the first completed unsuccessfully ( || ). For example, the sequence program1 input.txt > intermediate-results.txt && program2 intermediate-results.txt > results.txt will execute the second command only if previous commands have completed with a nonzero exit status. By contrast, program1 input.txt > intermediate-results.txt || echo \"warning: an error occurred\" will print the message if error has occurred. When a script ends with an exit that has no parameter, the exit status of the script is the exit status of the last command executed in the script (previous to the exit ). Challenge To test your understanding of && and || , we\u2019ll use two Unix commands that do nothing but return either exit success (true) or exit failure (false). Predict and check the outcome of the following commands: true echo $? false echo $? true && echo \"first command was a success\" true || echo \"first command was not a success\" false || echo \"first command was not a success\" false && echo \"first command was a success\" hint The $? variable represents the exit status of the previous command. Command Substitution \u00b6 Unix users like to have the Unix shell do work for them\u2014this is why shell expansions like wildcards and brace expansion exist. Another type of useful shell expansion is command substitution. Command substitution runs a Unix command inline and returns the output as a string that can be used in another command. This opens up a lot of useful possibilities. For example, if you want to include the results from executing a command into a text, you can type: echo \"There are $( grep -c '^>' SRR097977.fasta ) entries in my FASTA file.\" This command uses grep to count (the -c option stands for count) the number of lines matching the pattern. Using command substitution, we can calculate and return the number of FASTA entries directly into this string! Another example of using command substitution would be creating dated directories: mkdir results- $( date +%F ) %F - full date; same as %Y-%m-%d","title":"4. Streams, Redirection and Pipe"},{"location":"3_streams_red_pipe/#4-streams-redirection-and-pipe","text":"Lesson Objectives To be able to redirect streams of data in Unix. Solve problems by piping several Unix commands. Command substitution Bioinformatics data is often text-based and large. This is why Unix\u2019s philosophy of handling text streams is useful in bioinformatics: text streams allow us to do processing on a stream of data rather than holding it all in memory. Handling and redirecting the streams of data is essential skill in Unix. By default, both standard error and standard output of most unix programs go to your terminal screen. We can change this behavior (redirect the streams to a file) by using > or >> operators. The operator > redirects standard output to a file and overwrites any existing contents of the file, whereas >> appends to the file. If there isn\u2019t an existing file, both operators will create it before redirecting output to it. For example, to concatenate two FASTA files, we use cat command, but redirect the output to a file:","title":"4. Streams, Redirection and Pipe"},{"location":"3_streams_red_pipe/#output-redirection","text":"The shell4b_data directory contains the following fasta files: tb1.fasta tb1-protein.fasta tga1-protein.fasta We can use the cat command to view these files either one at a time: cat tb1-protein.fasta >teosinte-branched-1 protein LGVPSVKHMFPFCDSSSPMDLPLYQQLQLSPSSPKTDQSSSFYCYPCSPP FAAADASFPLSYQIGSAAAADATPPQAVINSPDLPVQALMDHAPAPATEL GACASGAEGSGASLDRAAAAARKDRHSKICTAGGMRDRRMRLSLDVARKF FALQDMLGFDKASKTVQWLLNTSKSAIQEIMADDASSECVEDGSSSLSVD GKHNPAEQLGGGGDQKPKGNCRGEGKKPAKASKAAATPKPPRKSANNAHQ VPDKETRAKARERARERTKEKHRMRWVKLASAIDVEAAAASVPSDRPSSN NLSHHSSLSMNMPCAAA cat tga1-protein.fasta >teosinte-glume-architecture-1 protein DSDCALSLLSAPANSSGIDVSRMVRPTEHVPMAQQPVVPGLQFGSASWFP RPQASTGGSFVPSCPAAVEGEQQLNAVLGPNDSEVSMNYGGMFHVGGGSG GGEGSSDGGT OR all at once with cat *.fasta We can also redirect the output to create a new file containing the sequence for both proteins: cat tb1-protein.fasta tga1-protein.fasta > zea-proteins.fasta Now we have a new file called zea-proteins.fasta . Let's check the contents: cat zea-proteins.fasta >teosinte-branched-1 protein LGVPSVKHMFPFCDSSSPMDLPLYQQLQLSPSSPKTDQSSSFYCYPCSPP FAAADASFPLSYQIGSAAAADATPPQAVINSPDLPVQALMDHAPAPATEL GACASGAEGSGASLDRAAAAARKDRHSKICTAGGMRDRRMRLSLDVARKF FALQDMLGFDKASKTVQWLLNTSKSAIQEIMADDASSECVEDGSSSLSVD GKHNPAEQLGGGGDQKPKGNCRGEGKKPAKASKAAATPKPPRKSANNAHQ VPDKETRAKARERARERTKEKHRMRWVKLASAIDVEAAAASVPSDRPSSN NLSHHSSLSMNMPCAAA >teosinte-glume-architecture-1 protein DSDCALSLLSAPANSSGIDVSRMVRPTEHVPMAQQPVVPGLQFGSASWFP RPQASTGGSFVPSCPAAVEGEQQLNAVLGPNDSEVSMNYGGMFHVGGGSG GGEGSSDGGT Capturing error messages cat tb1-protein.fasta mik.fasta >teosinte-branched-1 protein LGVPSVKHMFPFCDSSSPMDLPLYQQLQLSPSSPKTDQSSSFYCYPCSPP FAAADASFPLSYQIGSAAAADATPPQAVINSPDLPVQALMDHAPAPATEL GACASGAEGSGASLDRAAAAARKDRHSKICTAGGMRDRRMRLSLDVARKF FALQDMLGFDKASKTVQWLLNTSKSAIQEIMADDASSECVEDGSSSLSVD GKHNPAEQLGGGGDQKPKGNCRGEGKKPAKASKAAATPKPPRKSANNAHQ VPDKETRAKARERARERTKEKHRMRWVKLASAIDVEAAAASVPSDRPSSN NLSHHSSLSMNMPCAAA cat: mik.fasta: No such file or directory There are two different types of output there: \"standard output\" (the contents of the tb1-protein.fasta file) and standard error (the error message relating to the missing mik.fasta file). If we use the > operator to redirect the output, the standard output is captured, but the standard error is not - it is still printed to the screen. Let's check: cat tb1-protein.fasta mik.fasta > test.fasta cat: mik.fasta: No such file or directory The new file has been created and contains the standard output (contents of the file tb1-protein.fasta ): cat test.fasta >teosinte-branched-1 protein LGVPSVKHMFPFCDSSSPMDLPLYQQLQLSPSSPKTDQSSSFYCYPCSPP FAAADASFPLSYQIGSAAAADATPPQAVINSPDLPVQALMDHAPAPATEL GACASGAEGSGASLDRAAAAARKDRHSKICTAGGMRDRRMRLSLDVARKF FALQDMLGFDKASKTVQWLLNTSKSAIQEIMADDASSECVEDGSSSLSVD GKHNPAEQLGGGGDQKPKGNCRGEGKKPAKASKAAATPKPPRKSANNAHQ VPDKETRAKARERARERTKEKHRMRWVKLASAIDVEAAAASVPSDRPSSN NLSHHSSLSMNMPCAAA If we want to capture the standard error we use the (slightly unweildy) 2> operator: cat tb1-protein.fasta mik.fasta > test.fasta 2 > stderror.txt Descriptors File descriptor 2 represents standard error (other special file descriptors include 0 for standard input and 1 for standard output). Check the contents: cat stderror.txt cat: mik.fasta: No such file or directory Note that > will overwrite an existing file. We can use >> to add to a file instead of overwriting it: cat tga1-protein.fasta >> test.fasta cat test.fasta >teosinte-branched-1 protein LGVPSVKHMFPFCDSSSPMDLPLYQQLQLSPSSPKTDQSSSFYCYPCSPP FAAADASFPLSYQIGSAAAADATPPQAVINSPDLPVQALMDHAPAPATEL GACASGAEGSGASLDRAAAAARKDRHSKICTAGGMRDRRMRLSLDVARKF FALQDMLGFDKASKTVQWLLNTSKSAIQEIMADDASSECVEDGSSSLSVD GKHNPAEQLGGGGDQKPKGNCRGEGKKPAKASKAAATPKPPRKSANNAHQ VPDKETRAKARERARERTKEKHRMRWVKLASAIDVEAAAASVPSDRPSSN NLSHHSSLSMNMPCAAA >teosinte-glume-architecture-1 protein DSDCALSLLSAPANSSGIDVSRMVRPTEHVPMAQQPVVPGLQFGSASWFP RPQASTGGSFVPSCPAAVEGEQQLNAVLGPNDSEVSMNYGGMFHVGGGSG GGEGSSDGGT","title":"Output redirection"},{"location":"3_streams_red_pipe/#the-unix-pipe","text":"The pipe operator ( | ) passes the output from one command to another command as input. The following is an example of using a pipe with the grep command. Steps: Remove the header information for the sequence (line starts with \">\") Highlight any characters in the sequence that are not A, T, C or G. We will use grep to carry out the first step, and then use the pipe operator to pass the output to a second grep command to carry out the second step. Here is the full command: grep -v \"^>\" tb1.fasta | grep --color -i \"[^ATCG]\" Let's see what each piece does grep -v \"^>\" tb1.fasta The -v tells grep to search for all lines in the file tb1.fasta that do not contain a \">\" at the start ( ^ is a special character that denotes \"at the start of the line - we'll learn more about this later). grep --color -i \"[^ATCG]\" There are a few things going on here: --color : tells grep to highlight any matches -i : tells grep to ignore the case (i.e., will match lower or upper case) [^ATCG] : when ^ is used inside square brackets it has a different function - inverts the pattern, so that grep finds any letters that are not A, T, C or G. Let's run the code: grep -v \"^>\" tb1.fasta | grep --color -i \"[^ATCG]\" CCCCAAAGACGGACCAATCCAGCAGCTTCTACTGCTA Y CCATGCTCCCCTCCCTTCGCCGCCGCCGACGC What if we had just run the code for step 2 on the tb1.fasta file? grep --color -i \"[^ATCG]\" tb1.fasta","title":"The Unix pipe"},{"location":"3_streams_red_pipe/#combining-pipes-and-redirection","text":"grep -v \"^>\" tb1.fasta | grep --color -i \"[^ATCG]\" > non-atcg.txt cat non-atcg.txt since we are redirecting to a text file, the --color by itself will not record the colour information. We can achieve this by invoking always flag for --color .i.e.. grep -v \"^>\" tb1.fasta | grep --color = always -i \"[^ATCG]\" > non-atcg.txt","title":"Combining pipes and redirection"},{"location":"3_streams_red_pipe/#using-tee-to-capture-intermediate-outputs","text":"grep -v \"^>\" tb1.fasta | tee intermediate-file.txt | grep --color = always -i \"[^ATCG]\" > non-atcg.txt The file intermediate-file.txt will contain the output from grep -v \"^>\" tb1.fasta , but tee also passes that output through the pipe to the next grep command.","title":"Using tee to capture intermediate outputs"},{"location":"3_streams_red_pipe/#pipes-and-chains-and-long-running-processes-exit-status-programmatically-tell-whether-your-command-worked","text":"How do you know when they complete? How do you know if they successfully finished without an error? Unix programs exit with an exit status, which indicates whether a program terminated without a problem or with an error. By Unix standards, an exit status of 0 indicates the process ran successfully, and any nonzero status indicates some sort of error has occurred (and hopefully the program prints an understandable error message, too). The exit status isn\u2019t printed to the terminal, but your shell will set its value to a shell variable named $? . We can use the echo command to look at this variable\u2019s value after running a command: program input.txt > results.txt ; echo $? Exit statuses are useful because they allow us to programmatically chain commands together in the shell. A subsequent command in a chain is run conditionally on the last command\u2019s exit status. The shell provides two operators that implement this: one operator that runs the subsequent command only if the first command completed successfully ( && ), and one operator that runs the next command only if the first completed unsuccessfully ( || ). For example, the sequence program1 input.txt > intermediate-results.txt && program2 intermediate-results.txt > results.txt will execute the second command only if previous commands have completed with a nonzero exit status. By contrast, program1 input.txt > intermediate-results.txt || echo \"warning: an error occurred\" will print the message if error has occurred. When a script ends with an exit that has no parameter, the exit status of the script is the exit status of the last command executed in the script (previous to the exit ). Challenge To test your understanding of && and || , we\u2019ll use two Unix commands that do nothing but return either exit success (true) or exit failure (false). Predict and check the outcome of the following commands: true echo $? false echo $? true && echo \"first command was a success\" true || echo \"first command was not a success\" false || echo \"first command was not a success\" false && echo \"first command was a success\" hint The $? variable represents the exit status of the previous command.","title":"Pipes and Chains and Long running processes  : Exit Status (Programmatically Tell Whether Your Command Worked)"},{"location":"3_streams_red_pipe/#command-substitution","text":"Unix users like to have the Unix shell do work for them\u2014this is why shell expansions like wildcards and brace expansion exist. Another type of useful shell expansion is command substitution. Command substitution runs a Unix command inline and returns the output as a string that can be used in another command. This opens up a lot of useful possibilities. For example, if you want to include the results from executing a command into a text, you can type: echo \"There are $( grep -c '^>' SRR097977.fasta ) entries in my FASTA file.\" This command uses grep to count (the -c option stands for count) the number of lines matching the pattern. Using command substitution, we can calculate and return the number of FASTA entries directly into this string! Another example of using command substitution would be creating dated directories: mkdir results- $( date +%F ) %F - full date; same as %Y-%m-%d","title":"Command Substitution"},{"location":"4_inspectmanipluate/","text":"5. Inspecting and Manipulating Text Data with Unix Tools - Part 1 \u00b6 Lesson Objectives Inspect file/s with utilities such as head , less . Extracting and formatting tabular data. Magical grep use sort , uniq , join to manipulate the one or multiple files at once Many formats in bioinformatics are simple tabular plain-text files delimited by a character. The most common tabular plain-text file format used in bioinformatics is tab-delimited. Bioinformatics evolved to favor tab-delimited formats because of the convenience of working with these files using Unix tools. Tabular Plain-Text Data Formats Tabular plain-text data formats are used extensively in computing. The basic format is incredibly simple: each row (also known as a record) is kept on its own line, and each column (also known as a field) is separated by some delimiter. There are three flavors you will encounter: tab-delimited, comma-separated, and variable space-delimited. Inspecting data with head and tail \u00b6 Although cat command is an easy way for us to open and view the content of a file, it is not very practical to do so for a file with thousands of lines as it will exhaust the shell \"space\". Instead, large files should be inspected first and then manipulated accordingly. The first round of inspection can be done with head and tail command which prints the first 10 lines and the last 10 lines ( -n 10 ) of a a file, respectively. Let's use head and tail to inspect Mus_musculus.GRCm38.75_chr1.bed head Mus_musculus.GRCm38.75_chr1.bed Output 1 3054233 3054733 1 3054233 3054733 1 3054233 3054733 1 3102016 3102125 1 3102016 3102125 1 3102016 3102125 1 3205901 3671498 1 3205901 3216344 1 3213609 3216344 1 3205901 3207317 tail Mus_musculus.GRCm38.75_chr1.bed Output 1 195166217 195166390 1 195165745 195165851 1 195165748 195165851 1 195165745 195165747 1 195228278 195228398 1 195228278 195228398 1 195228278 195228398 1 195240910 195241007 1 195240910 195241007 1 195240910 195241007 Changing the number of lines printed for either of those commands can be done by passing -n <number_of_lines> flag .i.e. Over-ride the -n 10 default Try those commands with 0n 4 to print top 4 lines and bottom 4 lines head -n 4 Mus_musculus.GRCm38.75_chr1.bed tail -n 4 Mus_musculus.GRCm38.75_chr1.bed Exercise 4.1 Sometimes it\u2019s useful to see both the beginning and end of a file \u2014 for example, if we have a sorted BED file and we want to see the positions of the first feature and last feature. Can you figure out a way to use both head and tail in a single command to inspect first and last 2 lines of Mus_musculus.GRCm38.75_chr1.bed ? solution ( head -n 2 ; tail -n 2 ) < Mus_musculus.GRCm38.75_chr1.bed Exercise 4.2 We can also use tail to remove the header of a file. Normally the -n argument specifies how many of the last lines of a file to include, but if -n is given a number x preceded with a + sign (e.g., +x ), tail will start from the x th line. So to chop off a header, we start from the second line with -n+2 . Use the seq command to generate a file containing the numbers 1 to 10, and then use the tail command to chop off the first line. solution seq 10 > nums.txt cat nums.txt tail -n+2 nums.txt Extract summary information with wc \u00b6 The \"wc\" in the wc command which stands for \"word count\" - this command can count the numbers of words, lines , and characters in a file (take a note on the order). wc Mus_musculus.GRCm38.75_chr1.bed 81226 243678 1698545 Mus_musculus.GRCm38.75_chr1.bed Often, we only need to list the number of lines, which can be done by using the -l flag. It can be used as a sanity check - for example, to make sure an output has the same number of lines as the input, OR to check that a certain file format which depends on another format without losing overall data structure wasn't corrupted or over/under manipulated. Qu. Count the number of lines in Mus_musculus.GRCm38.75_chr1.bed and Mus_musculus.GRCm38.75_chr1.gtf . Anything out of the ordinary ? Although wc -l is the quickest way to count the number of lines in a file, it is not the most robust as it relies on the very bad assumption that \"data is well formatted\" For an example, if we are to create a file with 3 rows of data and then two empty lines by pressing Enter twice, cat > fool_wc.bed 1 100 2 200 3 300 Ctrl+D to end the edits started with cat > wc -l fool_wc.bed 5 fool_wc.bed This is a good place to bring in grep again which can be used to count the number of lines while excluding white-spaces (spaces, tabs ( t ) or newlines ( n )) grep -c \"[^ \\n\\t]\" fool_wc.bed 3 Using cut with column data and formatting tabular data with column \u00b6 When working with plain-text tabular data formats like tab-delimited and CSV files, we often need to extract specific columns from the original file or stream. For example, suppose we wanted to extract only the start positions (the second column) of the Mus_musculus.GRCm38.75_chr1.bed file. The simplest way to do this is with cut . cut -f 2 Mus_musculus.GRCm38.75_chr1.bed | head -n 3 3054233 3054233 3054233 -f argument is how we specify which columns to keep. It can be used to specify a range as well cut -f 2 -3 Mus_musculus.GRCm38.75_chr1.bed | head -n 3 3054233 3054733 3054233 3054733 3054233 3054733 Using cut , we can convert our GTF for Mus_musculus.GRCm38.75_chr1.gtf to a three-column tab-delimited file of genomic ranges (e.g., chromosome, start, and end position). We\u2019ll chop off the metadata rows using the grep command covered earlier and then use cut to extract the first, fourth, and fifth columns (chromosome, start, end): grep -v \"^#\" Mus_musculus.GRCm38.75_chr1.gtf | cut -f 1 ,4,5 | head -n 3 1 3054233 3054733 1 3054233 3054733 1 3054233 3054733 Note that although our three-column file of genomic positions looks like a BED-formatted file, it\u2019s not (due to subtle differences in genomic range formats). cut also allows us to specify the column delimiter character. So, if we were to come across a CSV file containing chromosome names, start positions, and end positions, we could select columns from it, too: As you may have noticed when working with tab-delimited files, it\u2019s not always easy to see which elements belong to a particular column. For example: grep -v \"^#\" Mus_musculus.GRCm38.75_chr1.gtf | cut -f 1 -8 | head -n 3 1 pseudogene gene 3054233 3054733 . + . 1 unprocessed_pseudogene transcript 3054233 3054733 . + . 1 unprocessed_pseudogene exon 3054233 3054733 . + . While tabs are a great delimiter in plain-text data files, our variable width data causes our columns to not stack up well. There\u2019s a fix for this in Unix: program column -t (the -t option tells column to treat data as a table). The column -t command produces neat columns that are much easier to read: grep -v \"^#\" Mus_musculus.GRCm38.75_chr1.gtf | cut -f 1 -8 | column -t | head -n 3 1 pseudogene gene 3054233 3054733 . + . 1 unprocessed_pseudogene transcript 3054233 3054733 . + . 1 unprocessed_pseudogene exon 3054233 3054733 . + . Note that you should only use column -t to visualize data in the terminal, not to reformat data to write to a file. Tab-delimited data is preferable to data delimited by a variable number of spaces, since it\u2019s easier for programs to parse. Like cut , column \u2019s default delimiter is the tab character ( \\t ). We can specify a different delimiter with the -s option. So, if we wanted to visualize the columns of the Mus_musculus.GRCm38.75_chr1_bed.csv file more easily, we could use: column -s \",\" -t Mus_musculus.GRCm38.75_chr1.bed | head -n 3 1 3054233 3054733 1 3054233 3054733 1 3054233 3054733 Counting the number of columns of a .gtf with grep and wc GTF Format has 9 columns 1 2 3 4 5 6 7 8 9 seqname source feature start end score strand frame attribute In theory, we should be able to use the following command to isolate the column headers and count the number grep -v \"^#\" Mus_musculus.GRCm38.75_chr1.gtf | head -1 | wc -w However, this will return 16 and the problem is with the attribute column which is a \"A semicolon-separated list of tag-value pairs, providing additional information about each feature.\" gene_id \"ENSMUSG00000090025\"; gene_name \"Gm16088\"; gene_source \"havana\"; gene_biotype \"pseudogene\"; Therefore, we need to translate* these values to a single \"new line\" with tr command and then count the number of lines than than the number of words grep -v \"^#\" Mus_musculus.GRCm38.75_chr1.gtf | head -1 | tr '\\t' '\\n' | wc -l Sorting Plain-Text Data with sort \u00b6 Very often we need to work with sorted plain-text data in bioinformatics. The two most common reasons to sort data are as follows: - Certain operations are much more efficient when performed on sorted data. - Sorting data is a prerequisite to finding all unique lines. sort is designed to work with plain-text data with columns. Create a test .bed file with few rows and use sort command without any arguments. cat > test_sort.bed input chr1 26 39 chr1 32 47 chr3 11 28 chr1 40 49 chr3 16 27 chr1 9 28 chr2 35 54 chr1 10 19 sort test_sort.bed Output chr1 10 19 chr1 26 39 chr1 32 47 chr1 40 49 chr1 9 28 chr2 35 54 chr3 11 28 chr3 16 27 sort without any arguments simply sorts a file alphanumerically by line. Because chromosome is the first column, sorting by line effectively groups chromosomes together, as these are \"ties\" in the sorted order. However, using sort \u2019s default of sorting alphanumerically by line and doesn\u2019t handle tabular data properly. There are two new features we need: The ability to sort by particular columns The ability to tell sort that certain columns are numeric values (and not alpha\u2010numeric text) sort has a simple syntax to do this. Let\u2019s look at how we\u2019d sort example.bed by chromosome (first column), and start position (second column): sort -k1,1 -k2,2n test_sort.bed Output chr1 9 28 chr1 10 19 chr1 26 39 chr1 32 47 chr1 40 49 chr2 35 54 chr3 11 28 chr3 16 27 Here, we specify the columns (and their order) we want to sort by as -k arguments. In technical terms, -k specifies the sorting keys and their order. Each -k argument takes a range of columns as start,end , so to sort by a single column we use start,start . In the preceding example, we first sorted by the first column (chromosome), as the first -k argument was -k1,1 . Sorting by the first column alone leads to many ties in rows with the same chromosomes (e.g., \u201cchr1\u201d and \u201cchr3\u201d). Adding a second -k argument with a different column tells sort how to break these ties. In our example, -k2,2n tells sort to sort by the second column (start position), treating this column as numerical data (because there\u2019s an n in -k2,2n ). The end result is that rows are grouped by chromosome and sorted by start position. Exercise 4.3 Mus_musculus.GRCm38.75_chr1_random.gtf file is Mus_musculus.GRCm38.75_chr1.gtf with permuted rows (and without a metadata header). Can you group rows by chromosome, and sort by position? If yes, append the output to a separate file. solution sort -k1,1 -k4,4n Mus_musculus.GRCm38.75_chr1_random.gtf > Mus_musculus.GRCm38.75_chr1_sorted.gtf Finding Unique Values with uniq \u00b6 uniq takes lines from a file or standard input stream and outputs all lines with consecutive duplicates removed. While this is a relatively simple functionality, you will use uniq very frequently in command-line data processing. cat letters.txt Output A A B C B C C C uniq letters.txt A B C B C As you can see, uniq does not return the unique values in letters.txt \u2014 it only removes consecutive duplicate lines (keeping one). If instead we did want to find all unique lines in a file, we would first sort all lines using sort so that all identical lines are grouped next to each other, and then run uniq . sort letters.txt | uniq A B C uniq with -c shows the counts of occurrences next to the unique lines. uniq -c letters.txt 2 A 1 B 1 C 1 B 3 C sort letters.txt | uniq -c 2 A 2 B 4 C Combined with other Unix tools like cut , grep and sort , uniq can be used to summarize columns of tabular data: grep -v \"^#\" Mus_musculus.GRCm38.75_chr1.gtf | cut -f3 | sort | uniq -c Output 25901 CDS 36128 exon 2027 gene 2290 start_codon 2299 stop_codon 4993 transcript 7588 UTR Count in order from most frequent to last grep -v \"^#\" Mus_musculus.GRCm38.75_chr1.gtf | cut -f3 | sort | uniq -c | sort -rn 36128 exon 25901 CDS 7588 UTR 4993 transcript 2299 stop_codon 2290 start_codon 2027 gene n and r represents numerical sort and reverse order (Or descending as the default as ascending) Joining two files with join \u00b6 Prepare Create a file named example_lengths.txt with following content chr1 58352 chr2 39521 chr3 24859 The Unix tool join is used to join different files together by a common column. For example, we may want to add chromosome lengths recorded in example_lengths.txt to example.bed BED file, we saw earlier. The files look like this: cat example.bed echo \"==========\" cat example_lengths.txt chr1 26 39 chr1 32 47 chr3 11 28 chr1 40 49 chr3 16 27 chr1 9 28 chr2 35 54 chr1 10 19 ========== chr1 58352 chr2 39521 chr3 24859 To do this, we need to join both of these tabular files by their common column, the one containing the chromosome names. But first, we first need to sort both files by the column to be joined on (join would not work otherwise): sort -k1,1 example.bed > ../data/example_sorted.bed sort -c -k1,1 example_lengths.txt # verifies is already sorted The basic syntax is join -1 <file_1_field> -2 <file_2_field> <file_1> <file_2> . So, with example.bed and example_lengths.txt this would be: join -1 1 -2 1 example_sorted.bed example_lengths.txt > example_with_lengths.txt cat example_with_lengths.txt There are many types of joins. For now, it\u2019s important that we make sure join is working as we expect. Our expectation is that this join should not lead to fewer rows than in our example.bed file. We can verify this with wc -l : wc -l example_sorted.bed example_with_lengths.txt 8 example_sorted.bed 8 example_with_lengths.txt 16 total However, look what happens if our second file, example_lengths.txt doesn\u2019t have the lengths for chr3 : head -n2 example_lengths.txt > example_lengths_alt.txt #truncate file join -1 1 -2 1 example_sorted.bed example_lengths_alt.txt chr1 10 19 58352 chr1 26 39 58352 chr1 32 47 58352 chr1 40 49 58352 chr1 9 28 58352 chr2 35 54 39521 join -1 1 -2 1 example_sorted.bed example_lengths_alt.txt | wc -l 6 Because chr3 is absent from example_lengths_alt.txt , our join omits rows from example_sorted.bed that do not have an entry in the first column of example_lengths_alt.txt. If we don\u2019t want this behavior, we can use option -a to include unpairable lines\u2014ones that do not have an entry in either file: join -1 1 -2 1 -a 1 example_sorted.bed example_lengths_alt.txt chr1 10 19 58352 chr1 26 39 58352 chr1 32 47 58352 chr1 40 49 58352 chr1 9 28 58352 chr2 35 54 39521 chr3 11 28 chr3 16 27 Back to homepage","title":"5. Inspecting and Manipulating Text Data with Unix Tools - Part 1"},{"location":"4_inspectmanipluate/#5-inspecting-and-manipulating-text-data-with-unix-tools-part-1","text":"Lesson Objectives Inspect file/s with utilities such as head , less . Extracting and formatting tabular data. Magical grep use sort , uniq , join to manipulate the one or multiple files at once Many formats in bioinformatics are simple tabular plain-text files delimited by a character. The most common tabular plain-text file format used in bioinformatics is tab-delimited. Bioinformatics evolved to favor tab-delimited formats because of the convenience of working with these files using Unix tools. Tabular Plain-Text Data Formats Tabular plain-text data formats are used extensively in computing. The basic format is incredibly simple: each row (also known as a record) is kept on its own line, and each column (also known as a field) is separated by some delimiter. There are three flavors you will encounter: tab-delimited, comma-separated, and variable space-delimited.","title":"5. Inspecting and Manipulating Text Data with Unix Tools - Part 1"},{"location":"4_inspectmanipluate/#inspecting-data-with-head-and-tail","text":"Although cat command is an easy way for us to open and view the content of a file, it is not very practical to do so for a file with thousands of lines as it will exhaust the shell \"space\". Instead, large files should be inspected first and then manipulated accordingly. The first round of inspection can be done with head and tail command which prints the first 10 lines and the last 10 lines ( -n 10 ) of a a file, respectively. Let's use head and tail to inspect Mus_musculus.GRCm38.75_chr1.bed head Mus_musculus.GRCm38.75_chr1.bed Output 1 3054233 3054733 1 3054233 3054733 1 3054233 3054733 1 3102016 3102125 1 3102016 3102125 1 3102016 3102125 1 3205901 3671498 1 3205901 3216344 1 3213609 3216344 1 3205901 3207317 tail Mus_musculus.GRCm38.75_chr1.bed Output 1 195166217 195166390 1 195165745 195165851 1 195165748 195165851 1 195165745 195165747 1 195228278 195228398 1 195228278 195228398 1 195228278 195228398 1 195240910 195241007 1 195240910 195241007 1 195240910 195241007 Changing the number of lines printed for either of those commands can be done by passing -n <number_of_lines> flag .i.e. Over-ride the -n 10 default Try those commands with 0n 4 to print top 4 lines and bottom 4 lines head -n 4 Mus_musculus.GRCm38.75_chr1.bed tail -n 4 Mus_musculus.GRCm38.75_chr1.bed Exercise 4.1 Sometimes it\u2019s useful to see both the beginning and end of a file \u2014 for example, if we have a sorted BED file and we want to see the positions of the first feature and last feature. Can you figure out a way to use both head and tail in a single command to inspect first and last 2 lines of Mus_musculus.GRCm38.75_chr1.bed ? solution ( head -n 2 ; tail -n 2 ) < Mus_musculus.GRCm38.75_chr1.bed Exercise 4.2 We can also use tail to remove the header of a file. Normally the -n argument specifies how many of the last lines of a file to include, but if -n is given a number x preceded with a + sign (e.g., +x ), tail will start from the x th line. So to chop off a header, we start from the second line with -n+2 . Use the seq command to generate a file containing the numbers 1 to 10, and then use the tail command to chop off the first line. solution seq 10 > nums.txt cat nums.txt tail -n+2 nums.txt","title":"Inspecting data with head and tail"},{"location":"4_inspectmanipluate/#extract-summary-information-with-wc","text":"The \"wc\" in the wc command which stands for \"word count\" - this command can count the numbers of words, lines , and characters in a file (take a note on the order). wc Mus_musculus.GRCm38.75_chr1.bed 81226 243678 1698545 Mus_musculus.GRCm38.75_chr1.bed Often, we only need to list the number of lines, which can be done by using the -l flag. It can be used as a sanity check - for example, to make sure an output has the same number of lines as the input, OR to check that a certain file format which depends on another format without losing overall data structure wasn't corrupted or over/under manipulated. Qu. Count the number of lines in Mus_musculus.GRCm38.75_chr1.bed and Mus_musculus.GRCm38.75_chr1.gtf . Anything out of the ordinary ? Although wc -l is the quickest way to count the number of lines in a file, it is not the most robust as it relies on the very bad assumption that \"data is well formatted\" For an example, if we are to create a file with 3 rows of data and then two empty lines by pressing Enter twice, cat > fool_wc.bed 1 100 2 200 3 300 Ctrl+D to end the edits started with cat > wc -l fool_wc.bed 5 fool_wc.bed This is a good place to bring in grep again which can be used to count the number of lines while excluding white-spaces (spaces, tabs ( t ) or newlines ( n )) grep -c \"[^ \\n\\t]\" fool_wc.bed 3","title":"Extract summary information with wc"},{"location":"4_inspectmanipluate/#using-cut-with-column-data-and-formatting-tabular-data-with-column","text":"When working with plain-text tabular data formats like tab-delimited and CSV files, we often need to extract specific columns from the original file or stream. For example, suppose we wanted to extract only the start positions (the second column) of the Mus_musculus.GRCm38.75_chr1.bed file. The simplest way to do this is with cut . cut -f 2 Mus_musculus.GRCm38.75_chr1.bed | head -n 3 3054233 3054233 3054233 -f argument is how we specify which columns to keep. It can be used to specify a range as well cut -f 2 -3 Mus_musculus.GRCm38.75_chr1.bed | head -n 3 3054233 3054733 3054233 3054733 3054233 3054733 Using cut , we can convert our GTF for Mus_musculus.GRCm38.75_chr1.gtf to a three-column tab-delimited file of genomic ranges (e.g., chromosome, start, and end position). We\u2019ll chop off the metadata rows using the grep command covered earlier and then use cut to extract the first, fourth, and fifth columns (chromosome, start, end): grep -v \"^#\" Mus_musculus.GRCm38.75_chr1.gtf | cut -f 1 ,4,5 | head -n 3 1 3054233 3054733 1 3054233 3054733 1 3054233 3054733 Note that although our three-column file of genomic positions looks like a BED-formatted file, it\u2019s not (due to subtle differences in genomic range formats). cut also allows us to specify the column delimiter character. So, if we were to come across a CSV file containing chromosome names, start positions, and end positions, we could select columns from it, too: As you may have noticed when working with tab-delimited files, it\u2019s not always easy to see which elements belong to a particular column. For example: grep -v \"^#\" Mus_musculus.GRCm38.75_chr1.gtf | cut -f 1 -8 | head -n 3 1 pseudogene gene 3054233 3054733 . + . 1 unprocessed_pseudogene transcript 3054233 3054733 . + . 1 unprocessed_pseudogene exon 3054233 3054733 . + . While tabs are a great delimiter in plain-text data files, our variable width data causes our columns to not stack up well. There\u2019s a fix for this in Unix: program column -t (the -t option tells column to treat data as a table). The column -t command produces neat columns that are much easier to read: grep -v \"^#\" Mus_musculus.GRCm38.75_chr1.gtf | cut -f 1 -8 | column -t | head -n 3 1 pseudogene gene 3054233 3054733 . + . 1 unprocessed_pseudogene transcript 3054233 3054733 . + . 1 unprocessed_pseudogene exon 3054233 3054733 . + . Note that you should only use column -t to visualize data in the terminal, not to reformat data to write to a file. Tab-delimited data is preferable to data delimited by a variable number of spaces, since it\u2019s easier for programs to parse. Like cut , column \u2019s default delimiter is the tab character ( \\t ). We can specify a different delimiter with the -s option. So, if we wanted to visualize the columns of the Mus_musculus.GRCm38.75_chr1_bed.csv file more easily, we could use: column -s \",\" -t Mus_musculus.GRCm38.75_chr1.bed | head -n 3 1 3054233 3054733 1 3054233 3054733 1 3054233 3054733 Counting the number of columns of a .gtf with grep and wc GTF Format has 9 columns 1 2 3 4 5 6 7 8 9 seqname source feature start end score strand frame attribute In theory, we should be able to use the following command to isolate the column headers and count the number grep -v \"^#\" Mus_musculus.GRCm38.75_chr1.gtf | head -1 | wc -w However, this will return 16 and the problem is with the attribute column which is a \"A semicolon-separated list of tag-value pairs, providing additional information about each feature.\" gene_id \"ENSMUSG00000090025\"; gene_name \"Gm16088\"; gene_source \"havana\"; gene_biotype \"pseudogene\"; Therefore, we need to translate* these values to a single \"new line\" with tr command and then count the number of lines than than the number of words grep -v \"^#\" Mus_musculus.GRCm38.75_chr1.gtf | head -1 | tr '\\t' '\\n' | wc -l","title":"Using cut with column data and formatting tabular data with column"},{"location":"4_inspectmanipluate/#sorting-plain-text-data-with-sort","text":"Very often we need to work with sorted plain-text data in bioinformatics. The two most common reasons to sort data are as follows: - Certain operations are much more efficient when performed on sorted data. - Sorting data is a prerequisite to finding all unique lines. sort is designed to work with plain-text data with columns. Create a test .bed file with few rows and use sort command without any arguments. cat > test_sort.bed input chr1 26 39 chr1 32 47 chr3 11 28 chr1 40 49 chr3 16 27 chr1 9 28 chr2 35 54 chr1 10 19 sort test_sort.bed Output chr1 10 19 chr1 26 39 chr1 32 47 chr1 40 49 chr1 9 28 chr2 35 54 chr3 11 28 chr3 16 27 sort without any arguments simply sorts a file alphanumerically by line. Because chromosome is the first column, sorting by line effectively groups chromosomes together, as these are \"ties\" in the sorted order. However, using sort \u2019s default of sorting alphanumerically by line and doesn\u2019t handle tabular data properly. There are two new features we need: The ability to sort by particular columns The ability to tell sort that certain columns are numeric values (and not alpha\u2010numeric text) sort has a simple syntax to do this. Let\u2019s look at how we\u2019d sort example.bed by chromosome (first column), and start position (second column): sort -k1,1 -k2,2n test_sort.bed Output chr1 9 28 chr1 10 19 chr1 26 39 chr1 32 47 chr1 40 49 chr2 35 54 chr3 11 28 chr3 16 27 Here, we specify the columns (and their order) we want to sort by as -k arguments. In technical terms, -k specifies the sorting keys and their order. Each -k argument takes a range of columns as start,end , so to sort by a single column we use start,start . In the preceding example, we first sorted by the first column (chromosome), as the first -k argument was -k1,1 . Sorting by the first column alone leads to many ties in rows with the same chromosomes (e.g., \u201cchr1\u201d and \u201cchr3\u201d). Adding a second -k argument with a different column tells sort how to break these ties. In our example, -k2,2n tells sort to sort by the second column (start position), treating this column as numerical data (because there\u2019s an n in -k2,2n ). The end result is that rows are grouped by chromosome and sorted by start position. Exercise 4.3 Mus_musculus.GRCm38.75_chr1_random.gtf file is Mus_musculus.GRCm38.75_chr1.gtf with permuted rows (and without a metadata header). Can you group rows by chromosome, and sort by position? If yes, append the output to a separate file. solution sort -k1,1 -k4,4n Mus_musculus.GRCm38.75_chr1_random.gtf > Mus_musculus.GRCm38.75_chr1_sorted.gtf","title":"Sorting Plain-Text Data with sort"},{"location":"4_inspectmanipluate/#finding-unique-values-with-uniq","text":"uniq takes lines from a file or standard input stream and outputs all lines with consecutive duplicates removed. While this is a relatively simple functionality, you will use uniq very frequently in command-line data processing. cat letters.txt Output A A B C B C C C uniq letters.txt A B C B C As you can see, uniq does not return the unique values in letters.txt \u2014 it only removes consecutive duplicate lines (keeping one). If instead we did want to find all unique lines in a file, we would first sort all lines using sort so that all identical lines are grouped next to each other, and then run uniq . sort letters.txt | uniq A B C uniq with -c shows the counts of occurrences next to the unique lines. uniq -c letters.txt 2 A 1 B 1 C 1 B 3 C sort letters.txt | uniq -c 2 A 2 B 4 C Combined with other Unix tools like cut , grep and sort , uniq can be used to summarize columns of tabular data: grep -v \"^#\" Mus_musculus.GRCm38.75_chr1.gtf | cut -f3 | sort | uniq -c Output 25901 CDS 36128 exon 2027 gene 2290 start_codon 2299 stop_codon 4993 transcript 7588 UTR Count in order from most frequent to last grep -v \"^#\" Mus_musculus.GRCm38.75_chr1.gtf | cut -f3 | sort | uniq -c | sort -rn 36128 exon 25901 CDS 7588 UTR 4993 transcript 2299 stop_codon 2290 start_codon 2027 gene n and r represents numerical sort and reverse order (Or descending as the default as ascending)","title":"Finding Unique Values with uniq"},{"location":"4_inspectmanipluate/#joining-two-files-with-join","text":"Prepare Create a file named example_lengths.txt with following content chr1 58352 chr2 39521 chr3 24859 The Unix tool join is used to join different files together by a common column. For example, we may want to add chromosome lengths recorded in example_lengths.txt to example.bed BED file, we saw earlier. The files look like this: cat example.bed echo \"==========\" cat example_lengths.txt chr1 26 39 chr1 32 47 chr3 11 28 chr1 40 49 chr3 16 27 chr1 9 28 chr2 35 54 chr1 10 19 ========== chr1 58352 chr2 39521 chr3 24859 To do this, we need to join both of these tabular files by their common column, the one containing the chromosome names. But first, we first need to sort both files by the column to be joined on (join would not work otherwise): sort -k1,1 example.bed > ../data/example_sorted.bed sort -c -k1,1 example_lengths.txt # verifies is already sorted The basic syntax is join -1 <file_1_field> -2 <file_2_field> <file_1> <file_2> . So, with example.bed and example_lengths.txt this would be: join -1 1 -2 1 example_sorted.bed example_lengths.txt > example_with_lengths.txt cat example_with_lengths.txt There are many types of joins. For now, it\u2019s important that we make sure join is working as we expect. Our expectation is that this join should not lead to fewer rows than in our example.bed file. We can verify this with wc -l : wc -l example_sorted.bed example_with_lengths.txt 8 example_sorted.bed 8 example_with_lengths.txt 16 total However, look what happens if our second file, example_lengths.txt doesn\u2019t have the lengths for chr3 : head -n2 example_lengths.txt > example_lengths_alt.txt #truncate file join -1 1 -2 1 example_sorted.bed example_lengths_alt.txt chr1 10 19 58352 chr1 26 39 58352 chr1 32 47 58352 chr1 40 49 58352 chr1 9 28 58352 chr2 35 54 39521 join -1 1 -2 1 example_sorted.bed example_lengths_alt.txt | wc -l 6 Because chr3 is absent from example_lengths_alt.txt , our join omits rows from example_sorted.bed that do not have an entry in the first column of example_lengths_alt.txt. If we don\u2019t want this behavior, we can use option -a to include unpairable lines\u2014ones that do not have an entry in either file: join -1 1 -2 1 -a 1 example_sorted.bed example_lengths_alt.txt chr1 10 19 58352 chr1 26 39 58352 chr1 32 47 58352 chr1 40 49 58352 chr1 9 28 58352 chr2 35 54 39521 chr3 11 28 chr3 16 27 Back to homepage","title":"Joining two files with join"},{"location":"5_inspectmanipulate2/","text":"6. Inspecting and Manipulating Text Data with Unix Tools - Part 2 \u00b6 Lesson Objectives insertion, deletion, search and replace(substitution) with sed Use specialised language awk to do a variety of text-processing tasks Quick overview of bioawk (an extension of awk to process common biological data formats) sed \u00b6 The s treamline ed itor or sed command is a stream editor that reads one or more text files, makes changes or edits according to editing script, and writes the results to standard output. First, we will discuss sed command with respect to search and replace function. Find and Replace \u00b6 Most common use of sed is to substitute text, matching a pattern. The syntax for doing this in sed is as follows: sed 'OPERATION/REGEXP/REPLACEMENT/FLAGS' FILENAME Here, / is the delimiter (you can also use _ (underscore), | (pipe) or : (colon) as delimiter as well) OPERATION specifies the action to be performed (sometimes if a condition is satisfied). The most common and widely used operation is s which does the substitution operation (other useful operators include y for transformation, i for insertion, d for deletion etc.). REGEXP and REPLACEMENT specify search term and the substitution term respectively for the operation that is being performed. FLAGS are additional parameters that control the operation. Some common FLAGS include: g replace all the instances of REGEXP with REPLACEMENT (globally) N where N is any number, to replace Nth instance of the REGEXP with REPLACEMENT p if substitution was made, then prints the new pattern space i ignores case for matching REGEXP w file If substitution was made, write out the result to the given file d when specified without REPLACEMENT , deletes the found REGEXP Some find and replace examples Find and replace all chr to chromosome in the example.bed file and append the the edit to a new file names example_chromosome.bed sed 's/chr/chromosome/g' example.bed > example_chromosome.bed Find and replace chr to chromosome , only if you also find 40 in the line sed '/40/s/chr/chromosome/g' example.bed > example_40.bed Find and replace directly on the input, but save an old version too sed -i.old 's/chr/chromosome/g' example.bed -i to edit files in-place instead of printing to standard output Print specific lines of the file To print a specific line you can use the address function. Note that by default, sed will stream the entire file, so when you are interested in specific lines only, you will have to suppress this feature using the option -n . print 5 th line of example.bed sed -n '5p' example.bed We can provide any number of additional lines to print using -e option. Let's print line 2 and 5, sed -n -e '2p' -e '5p' example.bed It also accepts range, using , . Let's print line 2-6, sed -n '2,6p' example.bed Also, we can create specific pattern, like multiples of a number using ~ . Let's print every tenth line of Mus_musculus.GRCm38.75_chr1.bed starting from 10, 20, 30.. to end of the file sed -n '10~10p' Mus_musculus.GRCm38.75_chr1.bed Exercise 4.4 Can you use the above ~ trick to extract all the odd numbered lines from Mus_musculus.GRCm38.75_chr1.bed and append the output to a new file odd_sed.bed One of the powerful feature is that we can combine these ranges or multiples in any fashion. Example: fastq files have header on first line and sequence in second, next two lines will have the quality and a blank extra line (four lines make one read). Sometimes we only need the sequence and header sed -n '1~4p;2~4p' SRR097977.fastq Sanity Check It's not a bad practice validate some of these commands by comparing the output from another command. For an example, above sed -n '1~4p;2~4p' SRR097977.fastq should print exactly half the number of lines in the file as it is removing two lines per read. Do a quick sanity check with sed -n '1~4p;2~4p' SRR097977.fastq | wc -l & cat SRR097977.fastq | wc -l We can use the above trick to convert the .fastq to .fasta sed -n '1~4p;2~4p' SRR097977.fastq | sed 's/^@/>/g' > SRR097977.fasta Let's wrap up sed with one more use case (a slightly complicated looking one). Let's say that we want capture all the transcript names from the last column (9 th column) from .gtf file. We can write something similar to: grep -v \"^#\" Mus_musculus.GRCm38.75_chr1.gtf | head -n 3 | sed -E 's/.*transcript_id \"([^\"]+)\".*/\\1/' -E option to enable POSIX Extended Regular Expressions (ERE) Output is not really what we are after, 1 pseudogene gene 3054233 3054733 . + . gene_id \"ENSMUSG00000090025\" ; gene_name \"Gm16088\" ; gene_source \"havana\" ; gene_biotype \"pseudogene\" ; ENSMUST00000160944 ENSMUST00000160944 The is due to sed default behaviour where it prints every line, making replacements to matching lines. .i.e Some lines of the last column of Mus_musculus.GRCm38.75_chr1.gtf don't contain transcript_id . So, sed prints the entire line rather than captured group. One way to solve this would be to use grep transcript_id before sed to only work with lines containing the string transcript_id . However, sed offers a cleaner way. First, disable sed from outputting all lines with -n ( can use --quiet or --silent as well .i.e. suppress automatic printing of pattern space ). Then, by appending p ( Print the current pattern space ) after the last slash sed will print all lines it\u2019s made a replacement on. The following is an illustration of -n used with p : grep -v \"^#\" Mus_musculus.GRCm38.75_chr1.gtf | head -n 3 | sed -E -n 's/.*transcript_id \"([^\"]+)\".*/\\1/p' In preparation for next lesson, rename example.bed.old back to example.bed mv example.bed.old example.bed Aho, Weinberger, Kernighan = AWK \u00b6 Awk is a scripting language used for manipulating data and generating reports. The awk command programming language requires no compiling and allows the user to use variables, numeric functions, string functions, and logical operators. Awk is a utility that enables a programmer to write tiny but effective programs. These take the form of statements that define text patterns that are to be searched for in each line of a document, and the action that is to be taken when a match is found within a line. Awk is mostly used for pattern scanning and processing. It searches one or more files to see if they contain lines that match with the specified patterns and then perform the associated actions. WHAT CAN WE DO WITH AWK? AWK Operations: Scans a file line by line Splits each input line into fields Compares input line/fields to pattern Performs action(s) on matched lines Useful For: Transform data files Produce formatted reports Programming Constructs: Format output lines Arithmetic and string operations Conditionals and loops There are two key parts for understanding the Awk language: how Awk processes records, and pattern-action pairs. The rest of the language is quite simple. Awk processes input data a record (line) at a time. Each record is composed of fields (column entries) that Awk automatically separates. Awk assigns the entire record to the variable $0, field one\u2019s value to $1, field two\u2019s value to $2, etc. We build Awk programs using one or more of the following structures: pattern { action } . Each pattern is an expression or regular expression pattern. In Awk lingo, these are pattern-action pairs and we can chain multiple pattern-action pairs together (separated by semicolons). If we omit the pattern, Awk will run the action on all records. If we omit the action but specify a pattern, Awk will print all records that match the pattern. Syntax : awk options 'selection_criteria {action}' input-file > output-file Options -f program-file : Reads the AWK program source from the file program-file, instead of from the first command line argument. -F fs : Use fs for the input field separator Default behaviour of awk is to print every line of data from the specified file. .i.e. mimics cat awk '{print}' example.bed Output chr1 26 39 chr1 32 47 chr3 11 28 chr1 40 49 chr3 16 27 chr1 9 28 chr2 35 54 chr1 10 19 Print lines which match the given pattern awk '/chr1/ {print}' example.bed chr1 26 39 chr1 32 47 chr1 40 49 chr1 9 28 chr1 10 19 awk can be used to mimic functionality of cut awk '{print $2 \"\\t\" $3}' example.bed Output 26 39 32 47 11 28 40 49 16 27 9 28 35 54 10 19 Here, we\u2019re making use of Awk\u2019s string concatenation. Two strings are concatenated if they are placed next to each other with no argument. So for each record, $2\"\\t\"$3 concatenates the second field, a tab character, and the third field. However, this is an instance where using cut is much simpler as the equivalent of above is cut -f 2,3 example.bed Let\u2019s look at how we can incorporate simple pattern matching. Suppose we wanted to write a filter that only output lines where the length of the feature (end position - start position) was greater than 18. Awk supports arithmetic with the standard operators + , - , * , / , % (remainder), and ^ (exponentiation). We can subtract within a pattern to calculate the length of a feature, and filter on that expression: awk '$3 - $2 > 18' example.bed chr1 9 28 chr2 35 54 awk Comparison and Logical operations Comparison Description a == b a is equal to b a != b a is not equal to b a < b a is less than b a > b a is greater than b a <= b a is less than or equal to b a >= b a is greater than or equal to b a ~ b a matches regular expression pattern b a !~ b a does not match regular expression pattern b a && b logical a and b a \\|\\| b logical or a and b !a not a (logical negation) We can also chain patterns, by using logical operators && (AND), || (OR), and ! (NOT). For example, if we wanted all lines on chromosome 1 with a length greater than 10: awk '$1 ~ /chr1/ && $3 - $2 > 10' example.bed chr1 26 39 chr1 32 47 chr1 9 28 First pattern, $1 ~ /chr1 specifies the regular expression (All Regular expressions are in slashes). We are mating the first field, $1 against the regular expression chr1 . Tilde ~ means match . To do the inverse of match , we can use !~ OR !($1 ~ /chr1/ Built-In Variables and special patterns In Awk Awk\u2019s built-in variables include the field variables $1 , $2 , $3 , and so on ( $0 is the entire line) \u2014 that break a line of text into individual words or pieces called fields. NR : keeps a current count of the number of input records. Remember that records are usually lines. Awk command performs the pattern/action statements once for each record in a file. NF : keeps a count of the number of fields within the current input record. FS : contains the field separator character which is used to divide fields on the input line. The default is \u201cwhite space\u201d, meaning space and tab characters. FS can be reassigned to another character (typically in BEGIN) to change the field separator. RS : stores the current record separator character. Since, by default, an input line is the input record, the default record separator character is a newline. OFS : stores the output field separator, which separates the fields when Awk prints them. The default is a blank space. Whenever print has several parameters separated with commas, it will print the value of OFS in between each parameter. ORS : stores the output record separator, which separates the output lines when Awk prints them. The default is a newline character. print automatically outputs the contents of ORS at the end of whatever it is given to print. Also, there are two special patterns BEGIN & END BEGIN - specifies what to do before the first record is read in. Useful to initialise and set up variables END - what to do after the last record's processing is complete. Useful to print data summaries ad the end of file processing Examples We can use NR to extract ranges of lines, too; for example, if we wanted to extract all lines between 3 and 5 (inclusive): awk 'NR >= 3 && NR <=5' example.bed chr3 11 28 chr1 40 49 chr3 16 27 suppose we wanted to calculate the mean feature length in example.bed. We would have to take the sum feature lengths, and then divide by the total number of records. We can do this with: awk 'BEGIN{s = 0}; {s += ($3-$2)}; END{ print \"mean: \" s/NR};' example.bed mean: 14 Info In this example, we\u2019ve initialized a variable s to 0 in BEGIN (variables you define do not need a dollar sign). Then, for each record we increment s by the length of the feature. At the end of the records, we print this sum s divided by the number of records NR , giving the mean. awk makes it easy to convert between bioinformatics files like BED and GTF. For example, we could generate a three-column BED file from Mus_muscu\u2010lus.GRCm38.75_chr1.gtf as follows: awk '!/^#/ { print $1 \"\\t\" $4-1 \"\\t\" $5}' Mus_musculus.GRCm38.75_chr1.gtf | head -n 3 1 3054232 3054733 1 3054232 3054733 1 3054232 3054733 awk also has a very useful data structure known as an associative array. Associative arrays behave like Python\u2019s dictionaries or hashes in other languages. We can create an associative array by simply assigning a value to a key. For example, suppose we wanted to count the number of features (third column) belonging to the gene \u201cLypla1.\u201d We could do this by incrementing their values in an associative array: awk '/Lypla1/ {feature[$3] += 1}; END {for (k in feature) print k \"\\t\" feature[k]}' Mus_musculus.GRCm38.75_chr1.gtf exon 69 CDS 56 UTR 24 gene 1 start_codon 5 stop_codon 5 transcript 9 bioawk \u00b6 bioawk is an extension of awk , adding the support of several common biological data formats, including optionally gzip'ed BED, GFF, SAM, VCF, FASTA/Q and TAB-delimited formats with column names. It also adds a few built-in functions and a command line option to use TAB as the input/output delimiter. When the new functionality is not used, bioawk is intended to behave exactly the same as the original BWK awk. The original awk requires a YACC-compatible parser generator (e.g. Byacc or Bison). bioawk further depends on zlib so as to work with gzip'd files. YACC A parser generator is a program that takes as input a specification of a syntax, and produces as output a procedure for recognizing that language. Historically, they are also called compiler-compilers. YACC (yet another compiler-compiler) is an LALR(LookAhead, Left-to-right, Rightmost derivation producer with 1 lookahead token) parser generator. YACC was originally designed for being complemented by Lex . Lex (A Lexical Analyzer Generator) helps write programs whose control flow is directed by instances of regular expressions in the input stream. It is well suited for editor-script type transformations and for segmenting input in preparation for a parsing routine. bioawk features It can automatically recognize some popular formats and will parse different features associated with those formats. The format option is passed to bioawk using -c arg flag. Here arg can be bed, sam, vcf, gff or fastx (for both fastq and FASTA). It can also deal with other types of table formats using the -c header option. When header is specified, the field names will used for variable names, thus greatly expanding the utility.` There are several built-in functions (other than the standard awk built-ins), that are specific to biological file formats. When a format is read with bioawk , the fields get automatically parsed. You can apply several functions on these variables to get the desired output. Let\u2019s say, we read fasta format, now we have $name and $seq that holds sequence name and sequence respectively. You can use the print function ( awk built-in) to print $name and $seq . You can also use bioawk built-in with the print function to get length, reverse complement etc by using '{print length($seq)}' . Other functions include reverse , revcomp , trimq , and , or , xor etc. Variables for each format For the -c you can either specify bed, sam, vcf, gff, fastx or header. bioawk will parse these variables for the respective format. If -c header is specified, the field names (first line) will be used as variables (spaces and special characters will be changed to under_score). bed sam vcf gff fastx chrom qname chrom seqname name start flag pos source seq end rname id feature qual name pos ref start comment score mapq alt end strand cigar qual score thickstart rnext filter filter thickend pnext info strand rgb tlen group blockcount seq attribute blocksizes qual blockstarts bioawk is not a default linux/unix utility. .i.e. Has to be installed. This is available as a module on NeSI HPC platforms which can be loaded with module load bioawk/1.0 The basic idea of Bioawk is that we specify what bioinformatics format we\u2019re working with, and Bioawk will automatically set variables for each field (just as regular Awk sets the columns of a tabular text file to $1, $1, $2, etc.). For Bioawk to set these fields, specify the format of the input file or stream with -c. Let\u2019s look at Bioawk\u2019s supported input formats and what variables these formats set: bioawk -c help bed ed: 1 :chrom 2 :start 3 :end 4 :name 5 :score 6 :strand 7 :thickstart 8 :thickend 9 :rgb 10 :blockcount 11 :blocksizes 12 :blockstarts sam: 1 :qname 2 :flag 3 :rname 4 :pos 5 :mapq 6 :cigar 7 :rnext 8 :pnext 9 :tlen 10 :seq 11 :qual vcf: 1 :chrom 2 :pos 3 :id 4 :ref 5 :alt 6 :qual 7 :filter 8 :info gff: 1 :seqname 2 :source 3 :feature 4 :start 5 :end 6 :score 7 :filter 8 :strand 9 :group 10 :attribute fastx: 1 :name 2 :seq 3 :qual 4 :comment As an example of how this works, let\u2019s read in example.bed and append a column with the length of the feature (end position - start position) for all protein coding genes: bioawk -c gff '$3 ~ /gene/ && $2 ~ /protein_coding/ {print $seqname,$end-$start}' ../data/Mus_musculus.GRCm38.75_chr1.gtf | head -n 4 1 465597 1 16807 1 5485 1 12533 Bioawk is also quite useful for processing FASTA/FASTQ files. For example, we could use it to turn a FASTQ file into a FASTA file: bioawk -c fastx '{print \">\"$name\"\\n\"$seq}' SRR097977.fastq | head -n 4 >SRR097977.1 TATTCTGCCATAATGAAATTCGCCACTTGTTAGTGT >SRR097977.2 GGTTACTCTTTTAACCTTGATGTTTCGACGCTGTAT Note that Bioawk detects whether to parse input as FASTQ or FASTA when we use -c fastx . Bioawk can also serve as a method of counting the number of FASTQ/FASTA entries: bioawk -c fastx 'END{print NR}' SRR097977.fastq Bioawk\u2019s function revcomp() can be used to reverse complement a sequence: bioawk -c fastx '{print \">\"$name\"\\n\"revcomp($seq)}' SRR097977.fastq | head -n 4 >SRR097977.1 ACACTAACAAGTGGCGAATTTCATTATGGCAGAATA >SRR097977.2 ATACAGCGTCGAAACATCAAGGTTAAAAGAGTAACC Back to homepage","title":"6. Inspecting and Manipulating Text Data with Unix Tools - Part 2"},{"location":"5_inspectmanipulate2/#6-inspecting-and-manipulating-text-data-with-unix-tools-part-2","text":"Lesson Objectives insertion, deletion, search and replace(substitution) with sed Use specialised language awk to do a variety of text-processing tasks Quick overview of bioawk (an extension of awk to process common biological data formats)","title":"6. Inspecting and Manipulating Text Data with Unix Tools - Part 2"},{"location":"5_inspectmanipulate2/#sed","text":"The s treamline ed itor or sed command is a stream editor that reads one or more text files, makes changes or edits according to editing script, and writes the results to standard output. First, we will discuss sed command with respect to search and replace function.","title":"sed"},{"location":"5_inspectmanipulate2/#find-and-replace","text":"Most common use of sed is to substitute text, matching a pattern. The syntax for doing this in sed is as follows: sed 'OPERATION/REGEXP/REPLACEMENT/FLAGS' FILENAME Here, / is the delimiter (you can also use _ (underscore), | (pipe) or : (colon) as delimiter as well) OPERATION specifies the action to be performed (sometimes if a condition is satisfied). The most common and widely used operation is s which does the substitution operation (other useful operators include y for transformation, i for insertion, d for deletion etc.). REGEXP and REPLACEMENT specify search term and the substitution term respectively for the operation that is being performed. FLAGS are additional parameters that control the operation. Some common FLAGS include: g replace all the instances of REGEXP with REPLACEMENT (globally) N where N is any number, to replace Nth instance of the REGEXP with REPLACEMENT p if substitution was made, then prints the new pattern space i ignores case for matching REGEXP w file If substitution was made, write out the result to the given file d when specified without REPLACEMENT , deletes the found REGEXP Some find and replace examples Find and replace all chr to chromosome in the example.bed file and append the the edit to a new file names example_chromosome.bed sed 's/chr/chromosome/g' example.bed > example_chromosome.bed Find and replace chr to chromosome , only if you also find 40 in the line sed '/40/s/chr/chromosome/g' example.bed > example_40.bed Find and replace directly on the input, but save an old version too sed -i.old 's/chr/chromosome/g' example.bed -i to edit files in-place instead of printing to standard output Print specific lines of the file To print a specific line you can use the address function. Note that by default, sed will stream the entire file, so when you are interested in specific lines only, you will have to suppress this feature using the option -n . print 5 th line of example.bed sed -n '5p' example.bed We can provide any number of additional lines to print using -e option. Let's print line 2 and 5, sed -n -e '2p' -e '5p' example.bed It also accepts range, using , . Let's print line 2-6, sed -n '2,6p' example.bed Also, we can create specific pattern, like multiples of a number using ~ . Let's print every tenth line of Mus_musculus.GRCm38.75_chr1.bed starting from 10, 20, 30.. to end of the file sed -n '10~10p' Mus_musculus.GRCm38.75_chr1.bed Exercise 4.4 Can you use the above ~ trick to extract all the odd numbered lines from Mus_musculus.GRCm38.75_chr1.bed and append the output to a new file odd_sed.bed One of the powerful feature is that we can combine these ranges or multiples in any fashion. Example: fastq files have header on first line and sequence in second, next two lines will have the quality and a blank extra line (four lines make one read). Sometimes we only need the sequence and header sed -n '1~4p;2~4p' SRR097977.fastq Sanity Check It's not a bad practice validate some of these commands by comparing the output from another command. For an example, above sed -n '1~4p;2~4p' SRR097977.fastq should print exactly half the number of lines in the file as it is removing two lines per read. Do a quick sanity check with sed -n '1~4p;2~4p' SRR097977.fastq | wc -l & cat SRR097977.fastq | wc -l We can use the above trick to convert the .fastq to .fasta sed -n '1~4p;2~4p' SRR097977.fastq | sed 's/^@/>/g' > SRR097977.fasta Let's wrap up sed with one more use case (a slightly complicated looking one). Let's say that we want capture all the transcript names from the last column (9 th column) from .gtf file. We can write something similar to: grep -v \"^#\" Mus_musculus.GRCm38.75_chr1.gtf | head -n 3 | sed -E 's/.*transcript_id \"([^\"]+)\".*/\\1/' -E option to enable POSIX Extended Regular Expressions (ERE) Output is not really what we are after, 1 pseudogene gene 3054233 3054733 . + . gene_id \"ENSMUSG00000090025\" ; gene_name \"Gm16088\" ; gene_source \"havana\" ; gene_biotype \"pseudogene\" ; ENSMUST00000160944 ENSMUST00000160944 The is due to sed default behaviour where it prints every line, making replacements to matching lines. .i.e Some lines of the last column of Mus_musculus.GRCm38.75_chr1.gtf don't contain transcript_id . So, sed prints the entire line rather than captured group. One way to solve this would be to use grep transcript_id before sed to only work with lines containing the string transcript_id . However, sed offers a cleaner way. First, disable sed from outputting all lines with -n ( can use --quiet or --silent as well .i.e. suppress automatic printing of pattern space ). Then, by appending p ( Print the current pattern space ) after the last slash sed will print all lines it\u2019s made a replacement on. The following is an illustration of -n used with p : grep -v \"^#\" Mus_musculus.GRCm38.75_chr1.gtf | head -n 3 | sed -E -n 's/.*transcript_id \"([^\"]+)\".*/\\1/p' In preparation for next lesson, rename example.bed.old back to example.bed mv example.bed.old example.bed","title":"Find and Replace"},{"location":"5_inspectmanipulate2/#aho-weinberger-kernighan-awk","text":"Awk is a scripting language used for manipulating data and generating reports. The awk command programming language requires no compiling and allows the user to use variables, numeric functions, string functions, and logical operators. Awk is a utility that enables a programmer to write tiny but effective programs. These take the form of statements that define text patterns that are to be searched for in each line of a document, and the action that is to be taken when a match is found within a line. Awk is mostly used for pattern scanning and processing. It searches one or more files to see if they contain lines that match with the specified patterns and then perform the associated actions. WHAT CAN WE DO WITH AWK? AWK Operations: Scans a file line by line Splits each input line into fields Compares input line/fields to pattern Performs action(s) on matched lines Useful For: Transform data files Produce formatted reports Programming Constructs: Format output lines Arithmetic and string operations Conditionals and loops There are two key parts for understanding the Awk language: how Awk processes records, and pattern-action pairs. The rest of the language is quite simple. Awk processes input data a record (line) at a time. Each record is composed of fields (column entries) that Awk automatically separates. Awk assigns the entire record to the variable $0, field one\u2019s value to $1, field two\u2019s value to $2, etc. We build Awk programs using one or more of the following structures: pattern { action } . Each pattern is an expression or regular expression pattern. In Awk lingo, these are pattern-action pairs and we can chain multiple pattern-action pairs together (separated by semicolons). If we omit the pattern, Awk will run the action on all records. If we omit the action but specify a pattern, Awk will print all records that match the pattern. Syntax : awk options 'selection_criteria {action}' input-file > output-file Options -f program-file : Reads the AWK program source from the file program-file, instead of from the first command line argument. -F fs : Use fs for the input field separator Default behaviour of awk is to print every line of data from the specified file. .i.e. mimics cat awk '{print}' example.bed Output chr1 26 39 chr1 32 47 chr3 11 28 chr1 40 49 chr3 16 27 chr1 9 28 chr2 35 54 chr1 10 19 Print lines which match the given pattern awk '/chr1/ {print}' example.bed chr1 26 39 chr1 32 47 chr1 40 49 chr1 9 28 chr1 10 19 awk can be used to mimic functionality of cut awk '{print $2 \"\\t\" $3}' example.bed Output 26 39 32 47 11 28 40 49 16 27 9 28 35 54 10 19 Here, we\u2019re making use of Awk\u2019s string concatenation. Two strings are concatenated if they are placed next to each other with no argument. So for each record, $2\"\\t\"$3 concatenates the second field, a tab character, and the third field. However, this is an instance where using cut is much simpler as the equivalent of above is cut -f 2,3 example.bed Let\u2019s look at how we can incorporate simple pattern matching. Suppose we wanted to write a filter that only output lines where the length of the feature (end position - start position) was greater than 18. Awk supports arithmetic with the standard operators + , - , * , / , % (remainder), and ^ (exponentiation). We can subtract within a pattern to calculate the length of a feature, and filter on that expression: awk '$3 - $2 > 18' example.bed chr1 9 28 chr2 35 54 awk Comparison and Logical operations Comparison Description a == b a is equal to b a != b a is not equal to b a < b a is less than b a > b a is greater than b a <= b a is less than or equal to b a >= b a is greater than or equal to b a ~ b a matches regular expression pattern b a !~ b a does not match regular expression pattern b a && b logical a and b a \\|\\| b logical or a and b !a not a (logical negation) We can also chain patterns, by using logical operators && (AND), || (OR), and ! (NOT). For example, if we wanted all lines on chromosome 1 with a length greater than 10: awk '$1 ~ /chr1/ && $3 - $2 > 10' example.bed chr1 26 39 chr1 32 47 chr1 9 28 First pattern, $1 ~ /chr1 specifies the regular expression (All Regular expressions are in slashes). We are mating the first field, $1 against the regular expression chr1 . Tilde ~ means match . To do the inverse of match , we can use !~ OR !($1 ~ /chr1/ Built-In Variables and special patterns In Awk Awk\u2019s built-in variables include the field variables $1 , $2 , $3 , and so on ( $0 is the entire line) \u2014 that break a line of text into individual words or pieces called fields. NR : keeps a current count of the number of input records. Remember that records are usually lines. Awk command performs the pattern/action statements once for each record in a file. NF : keeps a count of the number of fields within the current input record. FS : contains the field separator character which is used to divide fields on the input line. The default is \u201cwhite space\u201d, meaning space and tab characters. FS can be reassigned to another character (typically in BEGIN) to change the field separator. RS : stores the current record separator character. Since, by default, an input line is the input record, the default record separator character is a newline. OFS : stores the output field separator, which separates the fields when Awk prints them. The default is a blank space. Whenever print has several parameters separated with commas, it will print the value of OFS in between each parameter. ORS : stores the output record separator, which separates the output lines when Awk prints them. The default is a newline character. print automatically outputs the contents of ORS at the end of whatever it is given to print. Also, there are two special patterns BEGIN & END BEGIN - specifies what to do before the first record is read in. Useful to initialise and set up variables END - what to do after the last record's processing is complete. Useful to print data summaries ad the end of file processing Examples We can use NR to extract ranges of lines, too; for example, if we wanted to extract all lines between 3 and 5 (inclusive): awk 'NR >= 3 && NR <=5' example.bed chr3 11 28 chr1 40 49 chr3 16 27 suppose we wanted to calculate the mean feature length in example.bed. We would have to take the sum feature lengths, and then divide by the total number of records. We can do this with: awk 'BEGIN{s = 0}; {s += ($3-$2)}; END{ print \"mean: \" s/NR};' example.bed mean: 14 Info In this example, we\u2019ve initialized a variable s to 0 in BEGIN (variables you define do not need a dollar sign). Then, for each record we increment s by the length of the feature. At the end of the records, we print this sum s divided by the number of records NR , giving the mean. awk makes it easy to convert between bioinformatics files like BED and GTF. For example, we could generate a three-column BED file from Mus_muscu\u2010lus.GRCm38.75_chr1.gtf as follows: awk '!/^#/ { print $1 \"\\t\" $4-1 \"\\t\" $5}' Mus_musculus.GRCm38.75_chr1.gtf | head -n 3 1 3054232 3054733 1 3054232 3054733 1 3054232 3054733 awk also has a very useful data structure known as an associative array. Associative arrays behave like Python\u2019s dictionaries or hashes in other languages. We can create an associative array by simply assigning a value to a key. For example, suppose we wanted to count the number of features (third column) belonging to the gene \u201cLypla1.\u201d We could do this by incrementing their values in an associative array: awk '/Lypla1/ {feature[$3] += 1}; END {for (k in feature) print k \"\\t\" feature[k]}' Mus_musculus.GRCm38.75_chr1.gtf exon 69 CDS 56 UTR 24 gene 1 start_codon 5 stop_codon 5 transcript 9","title":"Aho, Weinberger, Kernighan = AWK"},{"location":"5_inspectmanipulate2/#bioawk","text":"bioawk is an extension of awk , adding the support of several common biological data formats, including optionally gzip'ed BED, GFF, SAM, VCF, FASTA/Q and TAB-delimited formats with column names. It also adds a few built-in functions and a command line option to use TAB as the input/output delimiter. When the new functionality is not used, bioawk is intended to behave exactly the same as the original BWK awk. The original awk requires a YACC-compatible parser generator (e.g. Byacc or Bison). bioawk further depends on zlib so as to work with gzip'd files. YACC A parser generator is a program that takes as input a specification of a syntax, and produces as output a procedure for recognizing that language. Historically, they are also called compiler-compilers. YACC (yet another compiler-compiler) is an LALR(LookAhead, Left-to-right, Rightmost derivation producer with 1 lookahead token) parser generator. YACC was originally designed for being complemented by Lex . Lex (A Lexical Analyzer Generator) helps write programs whose control flow is directed by instances of regular expressions in the input stream. It is well suited for editor-script type transformations and for segmenting input in preparation for a parsing routine. bioawk features It can automatically recognize some popular formats and will parse different features associated with those formats. The format option is passed to bioawk using -c arg flag. Here arg can be bed, sam, vcf, gff or fastx (for both fastq and FASTA). It can also deal with other types of table formats using the -c header option. When header is specified, the field names will used for variable names, thus greatly expanding the utility.` There are several built-in functions (other than the standard awk built-ins), that are specific to biological file formats. When a format is read with bioawk , the fields get automatically parsed. You can apply several functions on these variables to get the desired output. Let\u2019s say, we read fasta format, now we have $name and $seq that holds sequence name and sequence respectively. You can use the print function ( awk built-in) to print $name and $seq . You can also use bioawk built-in with the print function to get length, reverse complement etc by using '{print length($seq)}' . Other functions include reverse , revcomp , trimq , and , or , xor etc. Variables for each format For the -c you can either specify bed, sam, vcf, gff, fastx or header. bioawk will parse these variables for the respective format. If -c header is specified, the field names (first line) will be used as variables (spaces and special characters will be changed to under_score). bed sam vcf gff fastx chrom qname chrom seqname name start flag pos source seq end rname id feature qual name pos ref start comment score mapq alt end strand cigar qual score thickstart rnext filter filter thickend pnext info strand rgb tlen group blockcount seq attribute blocksizes qual blockstarts bioawk is not a default linux/unix utility. .i.e. Has to be installed. This is available as a module on NeSI HPC platforms which can be loaded with module load bioawk/1.0 The basic idea of Bioawk is that we specify what bioinformatics format we\u2019re working with, and Bioawk will automatically set variables for each field (just as regular Awk sets the columns of a tabular text file to $1, $1, $2, etc.). For Bioawk to set these fields, specify the format of the input file or stream with -c. Let\u2019s look at Bioawk\u2019s supported input formats and what variables these formats set: bioawk -c help bed ed: 1 :chrom 2 :start 3 :end 4 :name 5 :score 6 :strand 7 :thickstart 8 :thickend 9 :rgb 10 :blockcount 11 :blocksizes 12 :blockstarts sam: 1 :qname 2 :flag 3 :rname 4 :pos 5 :mapq 6 :cigar 7 :rnext 8 :pnext 9 :tlen 10 :seq 11 :qual vcf: 1 :chrom 2 :pos 3 :id 4 :ref 5 :alt 6 :qual 7 :filter 8 :info gff: 1 :seqname 2 :source 3 :feature 4 :start 5 :end 6 :score 7 :filter 8 :strand 9 :group 10 :attribute fastx: 1 :name 2 :seq 3 :qual 4 :comment As an example of how this works, let\u2019s read in example.bed and append a column with the length of the feature (end position - start position) for all protein coding genes: bioawk -c gff '$3 ~ /gene/ && $2 ~ /protein_coding/ {print $seqname,$end-$start}' ../data/Mus_musculus.GRCm38.75_chr1.gtf | head -n 4 1 465597 1 16807 1 5485 1 12533 Bioawk is also quite useful for processing FASTA/FASTQ files. For example, we could use it to turn a FASTQ file into a FASTA file: bioawk -c fastx '{print \">\"$name\"\\n\"$seq}' SRR097977.fastq | head -n 4 >SRR097977.1 TATTCTGCCATAATGAAATTCGCCACTTGTTAGTGT >SRR097977.2 GGTTACTCTTTTAACCTTGATGTTTCGACGCTGTAT Note that Bioawk detects whether to parse input as FASTQ or FASTA when we use -c fastx . Bioawk can also serve as a method of counting the number of FASTQ/FASTA entries: bioawk -c fastx 'END{print NR}' SRR097977.fastq Bioawk\u2019s function revcomp() can be used to reverse complement a sequence: bioawk -c fastx '{print \">\"$name\"\\n\"revcomp($seq)}' SRR097977.fastq | head -n 4 >SRR097977.1 ACACTAACAAGTGGCGAATTTCATTATGGCAGAATA >SRR097977.2 ATACAGCGTCGAAACATCAAGGTTAAAAGAGTAACC Back to homepage","title":"bioawk"},{"location":"6_automate_fileprocessing_find_xargs/","text":"7. Automating File-Processing with find and xargs \u00b6 In this section, we\u2019ll learn about a more powerful way to specify files matching some criteria using Unix find . We\u2019ll also see how files printed by find can be passed to another tool called xargs to create powerful Unix-based processing workflows. Suppose you have a program named analyse_fastq that takes multiple filenames through a standard process. If you wanted to run this program on all files with the suffix .fastq, you might run: ls *.fastq | analyse_fastq Fail Your shell expands this wildcard to all matching files in the current directory, and ls prints these filenames. Unfortunately, this leads to a common complication that makes ls and wildcards a fragile solution. Suppose your directory contains a filename called treatment 03.fastq . In this case, ls returns treatment 03.fastq along with other files. However, because files are separated by spaces, and this file contains a space, analyse_fastq will interpret treatment 03.fastq as two separate files, named treatment and 03.fastq . This problem crops up periodically in different ways, and it\u2019s necessary to be aware of when writing file-processing pipelines. Note that this does not occur with file globbing in arguments\u2014if analyse_fastq takes multiple files as arguments, your shell handles this properly: analyse_fastq *.fastq Here, shell automatically escapes the space in the filename treatment 03.fastq , so analyse_fastq will correctly receive the arguments treatment-02.fastq , treatment-03.fastq ,. The potential problem here is that there\u2019s a limit to the number of files that can be specified as arguments. The limit is high, but you can reach it with NGS data. In this case you may get a meassage: : cannot execute [Argument list too long] Solution Solution to both of the above problems is through find and xargs , as we will see in the following sections. Finding files with find \u00b6 Basic syntax for find is find path expression find Path specifies the starting directory for search. Expressions are how we describe which files we want to find to return Unlike ls , find is recursive (it will search through the directory structure). In fact, running find on a directory (without other arguments) is a quick way to see it\u2019s structure, e.g., find /nesi/project/nesi02659/ | head find: /nesi/project/nesi02659/ /nesi/project/nesi02659/.jupyter /nesi/project/nesi02659/.jupyter/share /nesi/project/nesi02659/.jupyter/share/jupyter /nesi/project/nesi02659/.jupyter/share/jupyter/nbconvert /nesi/project/nesi02659/.jupyter/share/jupyter/nbconvert/templates \u2018/nesi/project/nesi02659/.jupyter/share/jupyter/nbconvert/templates\u2019: Permission denied /nesi/project/nesi02659/.jupyter/share/jupyter/kernels /nesi/project/nesi02659/.jupyter/share/jupyter/kernels/sismonr /nesi/project/nesi02659/.jupyter/share/jupyter/kernels/sismonr/kernel.json /nesi/project/nesi02659/.jupyter/share/jupyter/kernels/sismonr/logo-64x64.png Argument -maxdepth limits the depth of the search: to search only within the current directory, use find -maxdepth 1 . Exercise 6.1 Create a small directory system as below in your current working directory mkdir -p hihi_project/ { data/raw,scripts,results } touch hihi_project/data/raw/hihi { A,B,C } _R { 1 ,2 } .fastq Run find hihi_project and examine the output Output hihi_project/ hihi_project/results hihi_project/data hihi_project/data/raw hihi_project/data/raw/hihiB_R1.fastq hihi_project/data/raw/hihiA_R1.fastq hihi_project/data/raw/hihiA_R2.fastq hihi_project/data/raw/hihiC_R1.fastq hihi_project/data/raw/hihiB_R2.fastq hihi_project/data/raw/hihiC_R2.fastq hihi_project/scripts Use find to print the names of all files matching the pattern \u201chihiB*fastq\u201d (e.g., FASTQ files from sample \u201cB\u201d, both read pairs): find hihi_project/data/raw/ -name \"hihiB*fastq\" This gives similar results to ls hihiB*fastq , as we\u2019d expect. The primary difference is that find reports results separated by new lines and, by default, find is recursive. Because we only want to return fastq files (and not directories with that matching name), we might want to limit our results using the -type option: There are numerous different types you can search for; the most commonly used are f for files, d for directories, and l for links. find hihi_project/data/raw/ -name \"hihiB*fastq\" -type f By default, find connects different parts of an expression with logical AND . The find command in this case returns results where the name matches \u201chihiB*fastq\u201d and is a file (type \u201cf \u201d). find also allows explicitly connecting different parts of an expression with different operators. If we want to get the names of all fastq files from samples A or C, we\u2019ll use the operator -or to chain expressions: find hihi_project/data/raw/ -name \"hihiA*fastq\" -or -name \"hihiC*fastq\" -type f Another way to select these files is with negation: Some bash versions will accept \"!\" as the flag for this where others will require -not find hihi_project/data/raw/ -type f -not -name \"hihiB*fastq\" Suppose you were sharing this project folder with a colleague and a file named hihiB_R1-temp.fastq was created by them in hihi_project/data/raw but you want to ignore it in your file querying: find hihi_project/data/raw/ -type f -not -name \"hihiB*fastq\" -and -not -name \"*-temp*\" find s -exec : Running Commands on find\u2019s Results \u00b6 Find\u2019s real strength in bioinformatics is that it allows you to run commands on every file that is returned by find, using - exec option. Continuing from our last example, suppose that a collaborator created numerous temporary files. Let\u2019s emulate this (in the hihi_project/data/raw/ ): (then ls ensure the -temp.fastq files were created) touch hihi_project/data/raw/hihi { A,C } _R { 1 ,2 } -temp.fastq Although we can delete these files with rm *-temp.fastq , using rm with a wildcard in a directory filled with important data files is too risky. Using find \u2019s -exec is a much safer way to delete these files. For example, let\u2019s use find -exec and rm to delete these temporary files: find hihi_project/data/raw/ -name \"*-temp.fastq\" -exec rm {} \\; Notice the (required!) semicolumn and curly brackets at the end of the command! . In one line, we\u2019re able to pragmatically identify and execute a command on files that match a certain pattern. With find and -exec , a daunting task like processing a directory of 100,000 text files with a program is simple. In general, find -exec is most appropriate for quick, simple tasks (like deleting files, changing permissions, etc.). For larger tasks, xargs is a better choice. xargs \u00b6 xargs reads data from standard input (stdin) and executes the command (supplied to it as an argument) one or more times based on the input read. Any spaces, tabs, and newlines in the input are treated as delimiters, while blank lines are ignored. If no command is specified, xargs executes echo . (Notice, that echo by itself does not read from standard input!) xargs allows us to take input from standard in, and use this input as arguments to another program, which allows us to build commands programmatically. Using find with xargs is much like find -exec , but with some added advantages that make xargs a better choice for larger tasks. Let\u2019s re-create our messy temporary file directory example again: .i.e Make sure to run ls after the touch command to verify the files were created. touch hihi_project/data/raw/hihi { A,C } _R { 1 ,2 } -temp.fastq xargs works by taking input from standard in and splitting it by spaces, tabs, and newlines into arguments. Then, these arguments are passed to the command supplied. For example, to emulate the behavior of find -exec with rm , we use xargs with rm : find hihi_project/data/raw -name \"*-temp.fastq\" | xargs rm xargs passes all arguments received through standard in to the supplied program (rm in this example). This works well for programs like rm , touch , mkdir , and others that take multiple arguments. However, other programs only take a single argument at a time. We can set how many arguments are passed to each command call with xargs \u2019s -n argument. For example, we could call rm four separate times (each on one file) with: touch hihi_project/data/raw/zmays { A,C } _R { 1 ,2 } -temp.fastq find hihi_project/data/raw -name \"*-temp.fastq\" | xargs -n 1 rm One big benefit of xargs is that it separates the process that specifies the files to operate on ( find ) from applying a command to these files (through xargs ). If we wanted to inspect a long list of files find returns before running rm on all files in this list, we could use: touch hihi_project/data/raw/hihi { A,C } _R { 1 ,2 } -temp.fastq find hihi_project/data/raw/ -name \"*-temp.fastq\" > ohno_filestodelete.txt cat ohno_filestodelete.txt cat ohno_filestodelete.txt | xargs rm Using xargs with Replacement Strings to Apply Commands to Files In addition to adding arguments at the end of the command, xargs can place them in predefined positions. This is done with the -I option and a placeholder string ({}). Suppose an imaginary program fastq_stat takes an input file through the option \u2013in, gathers FASTQ statistics information, and then writes a summary to the file specified by the \u2013out option. We may want our output filenames to be paired with our input filenames and have corresponding names. We can tackle this with find , xargs , and basename :","title":"7. Automating File-Processing with find and xargs"},{"location":"6_automate_fileprocessing_find_xargs/#7-automating-file-processing-with-find-and-xargs","text":"In this section, we\u2019ll learn about a more powerful way to specify files matching some criteria using Unix find . We\u2019ll also see how files printed by find can be passed to another tool called xargs to create powerful Unix-based processing workflows. Suppose you have a program named analyse_fastq that takes multiple filenames through a standard process. If you wanted to run this program on all files with the suffix .fastq, you might run: ls *.fastq | analyse_fastq Fail Your shell expands this wildcard to all matching files in the current directory, and ls prints these filenames. Unfortunately, this leads to a common complication that makes ls and wildcards a fragile solution. Suppose your directory contains a filename called treatment 03.fastq . In this case, ls returns treatment 03.fastq along with other files. However, because files are separated by spaces, and this file contains a space, analyse_fastq will interpret treatment 03.fastq as two separate files, named treatment and 03.fastq . This problem crops up periodically in different ways, and it\u2019s necessary to be aware of when writing file-processing pipelines. Note that this does not occur with file globbing in arguments\u2014if analyse_fastq takes multiple files as arguments, your shell handles this properly: analyse_fastq *.fastq Here, shell automatically escapes the space in the filename treatment 03.fastq , so analyse_fastq will correctly receive the arguments treatment-02.fastq , treatment-03.fastq ,. The potential problem here is that there\u2019s a limit to the number of files that can be specified as arguments. The limit is high, but you can reach it with NGS data. In this case you may get a meassage: : cannot execute [Argument list too long] Solution Solution to both of the above problems is through find and xargs , as we will see in the following sections.","title":"7. Automating File-Processing with find and xargs"},{"location":"6_automate_fileprocessing_find_xargs/#finding-files-with-find","text":"Basic syntax for find is find path expression find Path specifies the starting directory for search. Expressions are how we describe which files we want to find to return Unlike ls , find is recursive (it will search through the directory structure). In fact, running find on a directory (without other arguments) is a quick way to see it\u2019s structure, e.g., find /nesi/project/nesi02659/ | head find: /nesi/project/nesi02659/ /nesi/project/nesi02659/.jupyter /nesi/project/nesi02659/.jupyter/share /nesi/project/nesi02659/.jupyter/share/jupyter /nesi/project/nesi02659/.jupyter/share/jupyter/nbconvert /nesi/project/nesi02659/.jupyter/share/jupyter/nbconvert/templates \u2018/nesi/project/nesi02659/.jupyter/share/jupyter/nbconvert/templates\u2019: Permission denied /nesi/project/nesi02659/.jupyter/share/jupyter/kernels /nesi/project/nesi02659/.jupyter/share/jupyter/kernels/sismonr /nesi/project/nesi02659/.jupyter/share/jupyter/kernels/sismonr/kernel.json /nesi/project/nesi02659/.jupyter/share/jupyter/kernels/sismonr/logo-64x64.png Argument -maxdepth limits the depth of the search: to search only within the current directory, use find -maxdepth 1 . Exercise 6.1 Create a small directory system as below in your current working directory mkdir -p hihi_project/ { data/raw,scripts,results } touch hihi_project/data/raw/hihi { A,B,C } _R { 1 ,2 } .fastq Run find hihi_project and examine the output Output hihi_project/ hihi_project/results hihi_project/data hihi_project/data/raw hihi_project/data/raw/hihiB_R1.fastq hihi_project/data/raw/hihiA_R1.fastq hihi_project/data/raw/hihiA_R2.fastq hihi_project/data/raw/hihiC_R1.fastq hihi_project/data/raw/hihiB_R2.fastq hihi_project/data/raw/hihiC_R2.fastq hihi_project/scripts Use find to print the names of all files matching the pattern \u201chihiB*fastq\u201d (e.g., FASTQ files from sample \u201cB\u201d, both read pairs): find hihi_project/data/raw/ -name \"hihiB*fastq\" This gives similar results to ls hihiB*fastq , as we\u2019d expect. The primary difference is that find reports results separated by new lines and, by default, find is recursive. Because we only want to return fastq files (and not directories with that matching name), we might want to limit our results using the -type option: There are numerous different types you can search for; the most commonly used are f for files, d for directories, and l for links. find hihi_project/data/raw/ -name \"hihiB*fastq\" -type f By default, find connects different parts of an expression with logical AND . The find command in this case returns results where the name matches \u201chihiB*fastq\u201d and is a file (type \u201cf \u201d). find also allows explicitly connecting different parts of an expression with different operators. If we want to get the names of all fastq files from samples A or C, we\u2019ll use the operator -or to chain expressions: find hihi_project/data/raw/ -name \"hihiA*fastq\" -or -name \"hihiC*fastq\" -type f Another way to select these files is with negation: Some bash versions will accept \"!\" as the flag for this where others will require -not find hihi_project/data/raw/ -type f -not -name \"hihiB*fastq\" Suppose you were sharing this project folder with a colleague and a file named hihiB_R1-temp.fastq was created by them in hihi_project/data/raw but you want to ignore it in your file querying: find hihi_project/data/raw/ -type f -not -name \"hihiB*fastq\" -and -not -name \"*-temp*\"","title":"Finding files with find"},{"location":"6_automate_fileprocessing_find_xargs/#finds-exec-running-commands-on-finds-results","text":"Find\u2019s real strength in bioinformatics is that it allows you to run commands on every file that is returned by find, using - exec option. Continuing from our last example, suppose that a collaborator created numerous temporary files. Let\u2019s emulate this (in the hihi_project/data/raw/ ): (then ls ensure the -temp.fastq files were created) touch hihi_project/data/raw/hihi { A,C } _R { 1 ,2 } -temp.fastq Although we can delete these files with rm *-temp.fastq , using rm with a wildcard in a directory filled with important data files is too risky. Using find \u2019s -exec is a much safer way to delete these files. For example, let\u2019s use find -exec and rm to delete these temporary files: find hihi_project/data/raw/ -name \"*-temp.fastq\" -exec rm {} \\; Notice the (required!) semicolumn and curly brackets at the end of the command! . In one line, we\u2019re able to pragmatically identify and execute a command on files that match a certain pattern. With find and -exec , a daunting task like processing a directory of 100,000 text files with a program is simple. In general, find -exec is most appropriate for quick, simple tasks (like deleting files, changing permissions, etc.). For larger tasks, xargs is a better choice.","title":"finds -exec: Running Commands on find\u2019s Results"},{"location":"6_automate_fileprocessing_find_xargs/#xargs","text":"xargs reads data from standard input (stdin) and executes the command (supplied to it as an argument) one or more times based on the input read. Any spaces, tabs, and newlines in the input are treated as delimiters, while blank lines are ignored. If no command is specified, xargs executes echo . (Notice, that echo by itself does not read from standard input!) xargs allows us to take input from standard in, and use this input as arguments to another program, which allows us to build commands programmatically. Using find with xargs is much like find -exec , but with some added advantages that make xargs a better choice for larger tasks. Let\u2019s re-create our messy temporary file directory example again: .i.e Make sure to run ls after the touch command to verify the files were created. touch hihi_project/data/raw/hihi { A,C } _R { 1 ,2 } -temp.fastq xargs works by taking input from standard in and splitting it by spaces, tabs, and newlines into arguments. Then, these arguments are passed to the command supplied. For example, to emulate the behavior of find -exec with rm , we use xargs with rm : find hihi_project/data/raw -name \"*-temp.fastq\" | xargs rm xargs passes all arguments received through standard in to the supplied program (rm in this example). This works well for programs like rm , touch , mkdir , and others that take multiple arguments. However, other programs only take a single argument at a time. We can set how many arguments are passed to each command call with xargs \u2019s -n argument. For example, we could call rm four separate times (each on one file) with: touch hihi_project/data/raw/zmays { A,C } _R { 1 ,2 } -temp.fastq find hihi_project/data/raw -name \"*-temp.fastq\" | xargs -n 1 rm One big benefit of xargs is that it separates the process that specifies the files to operate on ( find ) from applying a command to these files (through xargs ). If we wanted to inspect a long list of files find returns before running rm on all files in this list, we could use: touch hihi_project/data/raw/hihi { A,C } _R { 1 ,2 } -temp.fastq find hihi_project/data/raw/ -name \"*-temp.fastq\" > ohno_filestodelete.txt cat ohno_filestodelete.txt cat ohno_filestodelete.txt | xargs rm Using xargs with Replacement Strings to Apply Commands to Files In addition to adding arguments at the end of the command, xargs can place them in predefined positions. This is done with the -I option and a placeholder string ({}). Suppose an imaginary program fastq_stat takes an input file through the option \u2013in, gathers FASTQ statistics information, and then writes a summary to the file specified by the \u2013out option. We may want our output filenames to be paired with our input filenames and have corresponding names. We can tackle this with find , xargs , and basename :","title":"xargs"},{"location":"puzzles/","text":"Puzzles \u00b6 Info Download data (and answers \ud83d\ude0a) wget -c puzzles_da.tar.gz https://github.com/GenomicsAotearoa/shell-for-bioinformatics/releases/download/v2.0/puzzles_da.tar.gz -O - | tar -xz Review the content of the puzzles_da directory There are two directories, data and answers Each filename has a unique four character id which corresponds to the puzzle/question (Described below) We recommend appending the solution to a file and compare the content of it with the expected output in answers directory How to compare your solution with the provided solution ( introducing two new commands cmp and printf ) #!/bin/bash if cmp --silent -- \"provided_answer.txt\" \"my_answer.txt\" ; then printf \"\\U1F60A SUCCESS \\n\" else printf \"\\U1F97A Almost there...Try again please\\n\" fi Transcribing DNA into RNA (tdir) An RNA string is a string formed from the alphabet containing 'A', 'C', 'G', and 'U'. Given a DNA string \\(t\\) corresponding to a coding strand, its transcribed RNA string \\(u\\) is formed by replacing all occurrences of 'T' in \\(t\\) with 'U' in \\(u\\) Given: A DNA string \\(t\\) having length at most 1000 nt. Return: The transcribed RNA string of \\(t\\) Solution bash R Python Julia C++ There are multiple ways to do this cat tdir_data.txt | tr T U awk '{gsub(/T/,\"U\");print}' tdir_data.txt sed 's/T/U/g' tdir_data.txt gsub ( \"T\" , \"U\" , readLines ( \"tdir_data.txt\" )) with open ( 'tdir_data.txt' , 'r' ) as f1 : dna = f1 . read () rna = dna . replace ( \"T\" , \"U\" ) print ( rna ) seq = open ( \"tdir_data.txt\" ) do file read ( file , String ) end seq = replace ( seq , \"T\" => \"U\" ) println ( seq ) // to compile, save the code to tdir.cpp: // g++ tdir.cpp -o tdir_cpp // to run: // cat tdir_data.txt | tdir_cpp > answer #include <iostream> using namespace std ; int main () { char nucleotide ; while ( cin >> nucleotide ) { if ( nucleotide == 'T' ) cout << 'U' ; else cout << nucleotide ; } return 0 ; } Complementing a Strand of DNA (csod) In DNA strings, symbols 'A' and 'T' are complements of each other, as are 'C' and 'G'. The reverse complement of a DNA string \\(s\\) is the string \\(s^{c}\\) formed by reversing the symbols of \\(s\\) then taking the complement of each symbol (e.g., the reverse complement of \"GTCA\" is \"TGAC\"). Given: A DNA string s of length at most 1000 bp. Return: The reverse complement \\(s^{c}\\) of \\(s\\) . Solution bash Python Julia Perl C++ There are multiple ways to do this rev csod_data.txt | tr ATCG TAGC rev csod_data.txt | sed 'y/ATCG/TAGC/' cat csod_data.txt | tr 'ACGT' 'TGCA' | rev For multi-line sequences tr -d \"\\n\" < data.txt | rev | tr ATCG TAGC For FASTA files grep -v \"^>\" r.fasta | tr -d \"\\n\" | rev | tr ATCG TAGC for N in open ( \"csod_data.txt\" , \"r\" ) . read ()[:: - 1 ]: for pair in [ \"GC\" , \"AT\" ]: if N in pair : print ( \"\" . join ( set ( N ) ^ set ( pair )), end = \"\" ) f = read ( open ( \"csod_data.txt\" ), String ) dict = Dict ( \"A\" => \"T\" , \"C\" => \"G\" , \"T\" => \"A\" , \"G\" => \"C\" ) for i in reverse ( f [ 1 : end - 1 ]) print ( dict [ string ( i )]) end println () $filename = 'csod_data.txt' ; open ( FILEN , $filename ); $dna = <FILEN> ; $rev = reverse $dna ; $rev =~ tr /ATCG/ TAGC / ; print $rev ; exit ; // to compile, save the code as csod.cpp // g++ csod.cpp -o csod_cpp // to run: // cat csod_data.txt | ./csod_cpp > answer #include <iostream> #include <vector> char complement ( const char c ) { switch ( c ) { case 'A' : return 'T' ; case 'T' : return 'A' ; case 'C' : return 'G' ; case 'G' : return 'C' ; } return 'X' ; } int main () { using std :: cin ; using std :: cout ; using std :: vector ; char nucleotide ; vector < char > DNAstring ; while ( cin >> nucleotide ) { DNAstring . push_back ( nucleotide ); } for ( int pos = DNAstring . size () -1 ; pos >= 0 ; -- pos ) { cout << complement ( DNAstring [ pos ]); } return 0 ; } Counting DNA Nucleotides (dnct) A string is simply an ordered collection of symbols selected from some alphabet and formed into a word; the length of a string is the number of symbols that it contains. An example of a length 21 DNA string (whose alphabet contains the symbols 'A', 'C', 'G', and 'T') is \"ATGCTTCAGAAAGGTCTTACG.\" Given: A DNA string \\(s\\) of length at most 1000 nt. Return: Four integers (separated by spaces) counting the respective number of times that the symbols 'A', 'C', 'G', and 'T' occur in \\(s\\) Solution bash Python Rust awk '{a=gsub(\"A\",\"\");c=gsub(\"C\",\"\");g=gsub(\"G\",\"\");t=gsub(\"T\",\"\")} END {print a,c,g,t}' /path/to/file x = $( cat dnct_data.txt ) ; for i in A C G T ; do y = ${ x //[^ $i ] } ; echo -n \" ${# y } \" ; done A bit more simpler solution in bash for i in A C G T ; do grep -o $i dnct_data.txt | wc -l ; done with open ( 'dnct_data.txt' ) as file : dataset = file . read () print ( dataset . count ( 'A' ), dataset . count ( 'C' ), dataset . count ( 'G' ), dataset . count ( 'T' )) // to compile, save the code as dnct.rs // rustc dnct.rs use std :: fs :: File ; use std :: io :: { Read , Write }; fn dna_parse ( in_str : & str ) -> ( i32 , i32 , i32 , i32 ) { let mut a_count = 0 ; let mut c_count = 0 ; let mut g_count = 0 ; let mut t_count = 0 ; for symbol in in_str . chars () { match symbol { 'A' | 'a' => a_count += 1 , 'C' | 'c' => c_count += 1 , 'G' | 'g' => g_count += 1 , 'T' | 't' => t_count += 1 , '\\r' | '\\n' => (), x => println! ( \"Unknown DNA symbol: {}\" , x . escape_debug ()) }; } ( a_count , c_count , g_count , t_count ) } fn main () -> std :: io :: Result < () > { // Get the input data let mut in_file = File :: open ( \"dnct_data.txt\" ) ? ; let mut contents = String :: new (); in_file . read_to_string ( & mut contents ) ? ; // Process the data let ( a , c , g , t ) = dna_parse ( & contents ); let out_str = format! ( \"{} {} {} {}\" , a , c , g , t ); // Export the data let mut out_file = File :: create ( \"dnct_answer.txt\" ) ? ; out_file . write_all ( out_str . as_bytes ()) ? ; Ok (()) } Maximum Matchings and RNA Secondary Structures (mmrs) Figure The graph theoretical analogue of the quandary stated in the introduction above is that if we have an RNA string \\(s\\) that does not have the same number of occurrences of 'C' as 'G' and the same number of occurrences of 'A' as 'U', then the bonding graph of \\(s\\) cannot possibly possess a perfect matching among its basepair edges. For example, see Figure 1 ; in fact, most bonding graphs will not contain a perfect matching. In light of this fact, we define a maximum matching in a graph as a matching containing as many edges as possible. See Figure 2 for three maximum matchings in graphs. A maximum matching of basepair edges will correspond to a way of forming as many base pairs as possible in an RNA string, as shown in Figure 3 . Given: An RNA string \\(s\\) of length at most 100. Return: The total possible number of maximum matchings of basepair edges in the bonding graph of \\(s\\) Solution bash Python #!/bin/bash dna = \" $( cat mmrs_data.txt | tail -n +2 | tr -d '\\n' ) \" count () { echo -n \" ${ dna //[^ $1 ]/ } \" | wc -c } matches () { local n1 = $( count $1 ) local n2 = $( count $2 ) if test $n2 -gt $n1 ; then local tmp = $n1 n1 = $n2 n2 = $tmp fi seq -s \\* $(( n1 - n2 + 1 )) $n1 } echo \" $( matches A U ) * $( matches C G ) \" | bc from Bio import SeqIO from scipy.special import perm with open ( 'mmrs_data.txt' , encoding = 'utf-8' ) as handle : rna = SeqIO . read ( handle , 'fasta' ) . seq au , cg = ( rna . count ( 'A' ), rna . count ( 'U' )), ( rna . count ( 'C' ), rna . count ( 'G' )) print ( perm ( max ( au ), min ( au ), exact = True ) * perm ( max ( cg ), min ( cg ), exact = True )) Computing GC Content (cgcc) The GC-content of a DNA string is given by the percentage of symbols in the string that are 'C' or 'G'. For example, the GC-content of \"AGCTATAG\" is 37.5%. Note that the reverse complement of any DNA string has the same GC-content. DNA strings must be labeled when they are consolidated into a database. A commonly used method of string labeling is called FASTA format. In this format, the string is introduced by a line that begins with '>', followed by some labeling information. Subsequent lines contain the string itself; the first line to begin with '>' indicates the label of the next string. In our implementation, a string in FASTA format will be labeled by the ID \"GAotearoa_xxxx\", where \"xxxx\" denotes a four-digit code between 0000 and 9999. Given: At most 10 DNA strings in FASTA format (of length at most 1 kbp each). Return: The ID of the string having the highest GC-content, followed by the GC-content of that string. Rosalind allows for a default error of 0.001 in all decimal answers unless otherwise stated; please see the note on absolute error below. Solution bash Perl R (with seqinr) Python Python (\ud83e\udd13 version) #/bin/bash awk -v RS = \">\" -v FS = \"\\n\" 'BEGIN {max_id=\"\"; max_gc=\"\";} \\ $0 !=\"\" { \\ gc=0; \\ l=0; \\ for(i=2;i<=NF;i++) { \\ gc += gsub(/[GC]/, \".\", $i); \\ l += length($i);} \\ if(max_gc < gc/l*100) {max_id=$1; max_gc=gc/l*100} \\ } \\ END {print max_id\"\\n\"max_gc;}' cgcc_data.txt perl - ne 'BEGIN{$/=\"\\n>\";$gc=-1}chomp;s/^>//;($id,$seq)=(split(/\\n/,$_,2));$seq=~s/\\n//g;$cgc=100*(($seq=~y/GC//)/length($seq));if($cgc>$gc){$gc=$cgc;$gcid=$id};END{print \"$gcid\\n$gc\\n\"}' data / cgcc_data . txt library ( seqinr ) fasta <- read.fasta ( \"cgcc_data.txt\" ) gc_content <- apply ( matrix ( names ( fasta )), 1 , function ( x ){ GC ( fasta [[ x ]])}) most_gc <- which ( gc_content == max ( gc_content )) #Result rbind ( names ( fasta )[ most_gc ], paste ( signif ( gc_content [ most_gc ], 4 ) * 100 , \"%\" , sep = \"\" )) from Bio import SeqIO from Bio.Seq import Seq from Bio.Alphabet import IUPAC from Bio.SeqUtils import GC max_seq = Seq ( 'tata' , IUPAC . unambiguous_dna ) max_gc = () for seq_record in SeqIO . parse ( 'cgcc_data.txt' , 'fasta' ): if GC ( seq_record . seq ) > GC ( max_seq ): max_id = seq_record . id max_seq = seq_record . seq max_gc = GC ( seq_record . seq ) print ( max_id ) print ( max_gc ) from Bio import SeqIO from Bio.SeqUtils import GC input_file = 'cgcc_data.txt' seq_dict = { seq_record . id : GC ( seq_record . seq ) for seq_record in SeqIO . parse ( input_file , \"fasta\" ) } max_gc = max ( seq_dict , key = seq_dict . get ) # find key of the dicionary's max value print ( max_gc ) print ( seq_dict [ max_gc ]) Counting Disease Carriers (cdcr) To model the Hardy-Weinberg principle, assume that we have a population of \\(N\\) diploid individuals. If an allele is in genetic equilibrium, then because mating is random, we may view the 2 \\(N\\) chromosomes as receiving their alleles uniformly. In other words, if there are m dominant alleles, then the probability of a selected chromosome exhibiting the dominant allele is simply \\(p=m/2N\\) Because the first assumption of genetic equilibrium states that the population is so large as to be ignored, we will assume that \\(N\\) is infinite, so that we only need to concern ourselves with the value of \\(p\\) Given: An array \\(A\\) for which \\(A[k]\\) represents the proportion of homozygous recessive individuals for the \\(k\\) -th Mendelian factor in a diploid population. Assume that the population is in genetic equilibrium for all factors. Return: An array \\(B\\) having the same length as \\(A\\) in which \\(B[k]\\) represents the probability that a randomly selected individual carries at least one copy of the recessive allele for the \\(k\\) -th factor. Solution bash cat cdcr_data.txt | tr \\ '\\n' | sed 's/.*/2 * sqrt(&) - &/' | bc -l","title":"Puzzles"},{"location":"puzzles/#puzzles","text":"Info Download data (and answers \ud83d\ude0a) wget -c puzzles_da.tar.gz https://github.com/GenomicsAotearoa/shell-for-bioinformatics/releases/download/v2.0/puzzles_da.tar.gz -O - | tar -xz Review the content of the puzzles_da directory There are two directories, data and answers Each filename has a unique four character id which corresponds to the puzzle/question (Described below) We recommend appending the solution to a file and compare the content of it with the expected output in answers directory How to compare your solution with the provided solution ( introducing two new commands cmp and printf ) #!/bin/bash if cmp --silent -- \"provided_answer.txt\" \"my_answer.txt\" ; then printf \"\\U1F60A SUCCESS \\n\" else printf \"\\U1F97A Almost there...Try again please\\n\" fi Transcribing DNA into RNA (tdir) An RNA string is a string formed from the alphabet containing 'A', 'C', 'G', and 'U'. Given a DNA string \\(t\\) corresponding to a coding strand, its transcribed RNA string \\(u\\) is formed by replacing all occurrences of 'T' in \\(t\\) with 'U' in \\(u\\) Given: A DNA string \\(t\\) having length at most 1000 nt. Return: The transcribed RNA string of \\(t\\) Solution bash R Python Julia C++ There are multiple ways to do this cat tdir_data.txt | tr T U awk '{gsub(/T/,\"U\");print}' tdir_data.txt sed 's/T/U/g' tdir_data.txt gsub ( \"T\" , \"U\" , readLines ( \"tdir_data.txt\" )) with open ( 'tdir_data.txt' , 'r' ) as f1 : dna = f1 . read () rna = dna . replace ( \"T\" , \"U\" ) print ( rna ) seq = open ( \"tdir_data.txt\" ) do file read ( file , String ) end seq = replace ( seq , \"T\" => \"U\" ) println ( seq ) // to compile, save the code to tdir.cpp: // g++ tdir.cpp -o tdir_cpp // to run: // cat tdir_data.txt | tdir_cpp > answer #include <iostream> using namespace std ; int main () { char nucleotide ; while ( cin >> nucleotide ) { if ( nucleotide == 'T' ) cout << 'U' ; else cout << nucleotide ; } return 0 ; } Complementing a Strand of DNA (csod) In DNA strings, symbols 'A' and 'T' are complements of each other, as are 'C' and 'G'. The reverse complement of a DNA string \\(s\\) is the string \\(s^{c}\\) formed by reversing the symbols of \\(s\\) then taking the complement of each symbol (e.g., the reverse complement of \"GTCA\" is \"TGAC\"). Given: A DNA string s of length at most 1000 bp. Return: The reverse complement \\(s^{c}\\) of \\(s\\) . Solution bash Python Julia Perl C++ There are multiple ways to do this rev csod_data.txt | tr ATCG TAGC rev csod_data.txt | sed 'y/ATCG/TAGC/' cat csod_data.txt | tr 'ACGT' 'TGCA' | rev For multi-line sequences tr -d \"\\n\" < data.txt | rev | tr ATCG TAGC For FASTA files grep -v \"^>\" r.fasta | tr -d \"\\n\" | rev | tr ATCG TAGC for N in open ( \"csod_data.txt\" , \"r\" ) . read ()[:: - 1 ]: for pair in [ \"GC\" , \"AT\" ]: if N in pair : print ( \"\" . join ( set ( N ) ^ set ( pair )), end = \"\" ) f = read ( open ( \"csod_data.txt\" ), String ) dict = Dict ( \"A\" => \"T\" , \"C\" => \"G\" , \"T\" => \"A\" , \"G\" => \"C\" ) for i in reverse ( f [ 1 : end - 1 ]) print ( dict [ string ( i )]) end println () $filename = 'csod_data.txt' ; open ( FILEN , $filename ); $dna = <FILEN> ; $rev = reverse $dna ; $rev =~ tr /ATCG/ TAGC / ; print $rev ; exit ; // to compile, save the code as csod.cpp // g++ csod.cpp -o csod_cpp // to run: // cat csod_data.txt | ./csod_cpp > answer #include <iostream> #include <vector> char complement ( const char c ) { switch ( c ) { case 'A' : return 'T' ; case 'T' : return 'A' ; case 'C' : return 'G' ; case 'G' : return 'C' ; } return 'X' ; } int main () { using std :: cin ; using std :: cout ; using std :: vector ; char nucleotide ; vector < char > DNAstring ; while ( cin >> nucleotide ) { DNAstring . push_back ( nucleotide ); } for ( int pos = DNAstring . size () -1 ; pos >= 0 ; -- pos ) { cout << complement ( DNAstring [ pos ]); } return 0 ; } Counting DNA Nucleotides (dnct) A string is simply an ordered collection of symbols selected from some alphabet and formed into a word; the length of a string is the number of symbols that it contains. An example of a length 21 DNA string (whose alphabet contains the symbols 'A', 'C', 'G', and 'T') is \"ATGCTTCAGAAAGGTCTTACG.\" Given: A DNA string \\(s\\) of length at most 1000 nt. Return: Four integers (separated by spaces) counting the respective number of times that the symbols 'A', 'C', 'G', and 'T' occur in \\(s\\) Solution bash Python Rust awk '{a=gsub(\"A\",\"\");c=gsub(\"C\",\"\");g=gsub(\"G\",\"\");t=gsub(\"T\",\"\")} END {print a,c,g,t}' /path/to/file x = $( cat dnct_data.txt ) ; for i in A C G T ; do y = ${ x //[^ $i ] } ; echo -n \" ${# y } \" ; done A bit more simpler solution in bash for i in A C G T ; do grep -o $i dnct_data.txt | wc -l ; done with open ( 'dnct_data.txt' ) as file : dataset = file . read () print ( dataset . count ( 'A' ), dataset . count ( 'C' ), dataset . count ( 'G' ), dataset . count ( 'T' )) // to compile, save the code as dnct.rs // rustc dnct.rs use std :: fs :: File ; use std :: io :: { Read , Write }; fn dna_parse ( in_str : & str ) -> ( i32 , i32 , i32 , i32 ) { let mut a_count = 0 ; let mut c_count = 0 ; let mut g_count = 0 ; let mut t_count = 0 ; for symbol in in_str . chars () { match symbol { 'A' | 'a' => a_count += 1 , 'C' | 'c' => c_count += 1 , 'G' | 'g' => g_count += 1 , 'T' | 't' => t_count += 1 , '\\r' | '\\n' => (), x => println! ( \"Unknown DNA symbol: {}\" , x . escape_debug ()) }; } ( a_count , c_count , g_count , t_count ) } fn main () -> std :: io :: Result < () > { // Get the input data let mut in_file = File :: open ( \"dnct_data.txt\" ) ? ; let mut contents = String :: new (); in_file . read_to_string ( & mut contents ) ? ; // Process the data let ( a , c , g , t ) = dna_parse ( & contents ); let out_str = format! ( \"{} {} {} {}\" , a , c , g , t ); // Export the data let mut out_file = File :: create ( \"dnct_answer.txt\" ) ? ; out_file . write_all ( out_str . as_bytes ()) ? ; Ok (()) } Maximum Matchings and RNA Secondary Structures (mmrs) Figure The graph theoretical analogue of the quandary stated in the introduction above is that if we have an RNA string \\(s\\) that does not have the same number of occurrences of 'C' as 'G' and the same number of occurrences of 'A' as 'U', then the bonding graph of \\(s\\) cannot possibly possess a perfect matching among its basepair edges. For example, see Figure 1 ; in fact, most bonding graphs will not contain a perfect matching. In light of this fact, we define a maximum matching in a graph as a matching containing as many edges as possible. See Figure 2 for three maximum matchings in graphs. A maximum matching of basepair edges will correspond to a way of forming as many base pairs as possible in an RNA string, as shown in Figure 3 . Given: An RNA string \\(s\\) of length at most 100. Return: The total possible number of maximum matchings of basepair edges in the bonding graph of \\(s\\) Solution bash Python #!/bin/bash dna = \" $( cat mmrs_data.txt | tail -n +2 | tr -d '\\n' ) \" count () { echo -n \" ${ dna //[^ $1 ]/ } \" | wc -c } matches () { local n1 = $( count $1 ) local n2 = $( count $2 ) if test $n2 -gt $n1 ; then local tmp = $n1 n1 = $n2 n2 = $tmp fi seq -s \\* $(( n1 - n2 + 1 )) $n1 } echo \" $( matches A U ) * $( matches C G ) \" | bc from Bio import SeqIO from scipy.special import perm with open ( 'mmrs_data.txt' , encoding = 'utf-8' ) as handle : rna = SeqIO . read ( handle , 'fasta' ) . seq au , cg = ( rna . count ( 'A' ), rna . count ( 'U' )), ( rna . count ( 'C' ), rna . count ( 'G' )) print ( perm ( max ( au ), min ( au ), exact = True ) * perm ( max ( cg ), min ( cg ), exact = True )) Computing GC Content (cgcc) The GC-content of a DNA string is given by the percentage of symbols in the string that are 'C' or 'G'. For example, the GC-content of \"AGCTATAG\" is 37.5%. Note that the reverse complement of any DNA string has the same GC-content. DNA strings must be labeled when they are consolidated into a database. A commonly used method of string labeling is called FASTA format. In this format, the string is introduced by a line that begins with '>', followed by some labeling information. Subsequent lines contain the string itself; the first line to begin with '>' indicates the label of the next string. In our implementation, a string in FASTA format will be labeled by the ID \"GAotearoa_xxxx\", where \"xxxx\" denotes a four-digit code between 0000 and 9999. Given: At most 10 DNA strings in FASTA format (of length at most 1 kbp each). Return: The ID of the string having the highest GC-content, followed by the GC-content of that string. Rosalind allows for a default error of 0.001 in all decimal answers unless otherwise stated; please see the note on absolute error below. Solution bash Perl R (with seqinr) Python Python (\ud83e\udd13 version) #/bin/bash awk -v RS = \">\" -v FS = \"\\n\" 'BEGIN {max_id=\"\"; max_gc=\"\";} \\ $0 !=\"\" { \\ gc=0; \\ l=0; \\ for(i=2;i<=NF;i++) { \\ gc += gsub(/[GC]/, \".\", $i); \\ l += length($i);} \\ if(max_gc < gc/l*100) {max_id=$1; max_gc=gc/l*100} \\ } \\ END {print max_id\"\\n\"max_gc;}' cgcc_data.txt perl - ne 'BEGIN{$/=\"\\n>\";$gc=-1}chomp;s/^>//;($id,$seq)=(split(/\\n/,$_,2));$seq=~s/\\n//g;$cgc=100*(($seq=~y/GC//)/length($seq));if($cgc>$gc){$gc=$cgc;$gcid=$id};END{print \"$gcid\\n$gc\\n\"}' data / cgcc_data . txt library ( seqinr ) fasta <- read.fasta ( \"cgcc_data.txt\" ) gc_content <- apply ( matrix ( names ( fasta )), 1 , function ( x ){ GC ( fasta [[ x ]])}) most_gc <- which ( gc_content == max ( gc_content )) #Result rbind ( names ( fasta )[ most_gc ], paste ( signif ( gc_content [ most_gc ], 4 ) * 100 , \"%\" , sep = \"\" )) from Bio import SeqIO from Bio.Seq import Seq from Bio.Alphabet import IUPAC from Bio.SeqUtils import GC max_seq = Seq ( 'tata' , IUPAC . unambiguous_dna ) max_gc = () for seq_record in SeqIO . parse ( 'cgcc_data.txt' , 'fasta' ): if GC ( seq_record . seq ) > GC ( max_seq ): max_id = seq_record . id max_seq = seq_record . seq max_gc = GC ( seq_record . seq ) print ( max_id ) print ( max_gc ) from Bio import SeqIO from Bio.SeqUtils import GC input_file = 'cgcc_data.txt' seq_dict = { seq_record . id : GC ( seq_record . seq ) for seq_record in SeqIO . parse ( input_file , \"fasta\" ) } max_gc = max ( seq_dict , key = seq_dict . get ) # find key of the dicionary's max value print ( max_gc ) print ( seq_dict [ max_gc ]) Counting Disease Carriers (cdcr) To model the Hardy-Weinberg principle, assume that we have a population of \\(N\\) diploid individuals. If an allele is in genetic equilibrium, then because mating is random, we may view the 2 \\(N\\) chromosomes as receiving their alleles uniformly. In other words, if there are m dominant alleles, then the probability of a selected chromosome exhibiting the dominant allele is simply \\(p=m/2N\\) Because the first assumption of genetic equilibrium states that the population is so large as to be ignored, we will assume that \\(N\\) is infinite, so that we only need to concern ourselves with the value of \\(p\\) Given: An array \\(A\\) for which \\(A[k]\\) represents the proportion of homozygous recessive individuals for the \\(k\\) -th Mendelian factor in a diploid population. Assume that the population is in genetic equilibrium for all factors. Return: An array \\(B\\) having the same length as \\(A\\) in which \\(B[k]\\) represents the probability that a randomly selected individual carries at least one copy of the recessive allele for the \\(k\\) -th factor. Solution bash cat cdcr_data.txt | tr \\ '\\n' | sed 's/.*/2 * sqrt(&) - &/' | bc -l","title":"Puzzles"},{"location":"supplementary_1/","text":"Escaping \u00b6 Escaping is a method of quoting single characters. The escape (\\) preceding a character tells the shell to interpret that character literally. Warning With certain commands and utilities, such as echo and sed , escaping a character may have the opposite effect - it can toggle on a special meaning for that character. Special meanings of certain escaped characters Character Meaning \\n newline \\r return \\t tab \\v vertical tab \\b backspace \\a alert (beep or flash) 0xx translates to the octal ASCII equivalent of 0nn, where nn is a string of digits Special Characters \u00b6 Special Characters Character Meaning # Comments . Lines beginning with a # (with the exception of #! ) are comments and will not be executed. ; Command separator [semicolon] . Permits putting two or more commands on the same line. ;; Terminator in a case option [double semicolon] . \" \"partial quoting [double quote] . \"STRING\" preserves (from interpretation) most of the special characters within STRING ' full quoting[single quote] 'STRING' preserves all special characters within STRING . This is a stronger form of quoting than \"STRING\" , comma operator . The comma operator [1] links together a series of arithmetic operations. All are evaluated, but only the last one is returned. ,, , Lowercase conversion in parameter substitution \\ escape [backslash] . A quoting mechanism for single characters. / Filename path separator [forward slash] . Separates the components of a filename.This is also the division arithmetic operator. ` command substitution . The command construct makes available the output of command for assignment to a variable. This is also known as backquotes or backticks. : null command [colon] . This is the shell equivalent of a \"NOP\" (no op, a do-nothing operation). It may be considered a synonym for the shell builtin true ! reverse (or negate) the sense of a test or exit status [bang]. The ! operator inverts the exit status of the command to which it is applied ? test operator . Within certain expressions, the ? indicates a test for a condition. $ Variable substitution (contents of a variable). ${} Parameter substitution $? Exit status variable $$ process ID variable () command group {} Block of code [curly brackets] . Also referred to as an inline group, this construct, in effect, creates an anonymous function (a function without a name). {} \\; pathname . Mostly used in find constructs. This is not a shell builtin. >| force redirection (even if the noclobber option is set). This will forcibly overwrite an existing file. || OR logical operator . In a test construct, the & Run job in background . A command followed by an & will run in the background. && AND logical operator . In a test construct, the && operator causes a return of 0 (success) only if both the linked test conditions are true. ~+ current working directory. This corresponds to the $PWD internal variable. ~- previous working directory . This corresponds to the $OLDPWD internal variable. =~ regular expression match. This operator was introduced with version 3 of Bash. ^ beginning-of-line . In a regular expression, a \"^\" addresses the beginning of a line of text. ^, ^^ Uppercase conversion in parameter substitution","title":"Supplementary 1"},{"location":"supplementary_1/#escaping","text":"Escaping is a method of quoting single characters. The escape (\\) preceding a character tells the shell to interpret that character literally. Warning With certain commands and utilities, such as echo and sed , escaping a character may have the opposite effect - it can toggle on a special meaning for that character. Special meanings of certain escaped characters Character Meaning \\n newline \\r return \\t tab \\v vertical tab \\b backspace \\a alert (beep or flash) 0xx translates to the octal ASCII equivalent of 0nn, where nn is a string of digits","title":"Escaping"},{"location":"supplementary_1/#special-characters","text":"Special Characters Character Meaning # Comments . Lines beginning with a # (with the exception of #! ) are comments and will not be executed. ; Command separator [semicolon] . Permits putting two or more commands on the same line. ;; Terminator in a case option [double semicolon] . \" \"partial quoting [double quote] . \"STRING\" preserves (from interpretation) most of the special characters within STRING ' full quoting[single quote] 'STRING' preserves all special characters within STRING . This is a stronger form of quoting than \"STRING\" , comma operator . The comma operator [1] links together a series of arithmetic operations. All are evaluated, but only the last one is returned. ,, , Lowercase conversion in parameter substitution \\ escape [backslash] . A quoting mechanism for single characters. / Filename path separator [forward slash] . Separates the components of a filename.This is also the division arithmetic operator. ` command substitution . The command construct makes available the output of command for assignment to a variable. This is also known as backquotes or backticks. : null command [colon] . This is the shell equivalent of a \"NOP\" (no op, a do-nothing operation). It may be considered a synonym for the shell builtin true ! reverse (or negate) the sense of a test or exit status [bang]. The ! operator inverts the exit status of the command to which it is applied ? test operator . Within certain expressions, the ? indicates a test for a condition. $ Variable substitution (contents of a variable). ${} Parameter substitution $? Exit status variable $$ process ID variable () command group {} Block of code [curly brackets] . Also referred to as an inline group, this construct, in effect, creates an anonymous function (a function without a name). {} \\; pathname . Mostly used in find constructs. This is not a shell builtin. >| force redirection (even if the noclobber option is set). This will forcibly overwrite an existing file. || OR logical operator . In a test construct, the & Run job in background . A command followed by an & will run in the background. && AND logical operator . In a test construct, the && operator causes a return of 0 (success) only if both the linked test conditions are true. ~+ current working directory. This corresponds to the $PWD internal variable. ~- previous working directory . This corresponds to the $OLDPWD internal variable. =~ regular expression match. This operator was introduced with version 3 of Bash. ^ beginning-of-line . In a regular expression, a \"^\" addresses the beginning of a line of text. ^, ^^ Uppercase conversion in parameter substitution","title":"Special Characters"}]}